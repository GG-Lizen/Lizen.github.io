
[{"content":"","date":"20 June 2025","externalUrl":null,"permalink":"/byteglow/","section":"ByteGlow","summary":"","title":"ByteGlow","type":"page"},{"content":" LangGraph # LangGraph，它是一个Python库 。其作用是构建有状态、多操作的大语言模型（LLM）应用程序，用于创建智能体（agent）和组合智能体（multi-agent）流程。 与其他LLM应用框架相比，LangGraph有核心优势：\n持久执行：构建能够在出现故障时持续运行并长时间工作的智能体，可自动从停止的位置精确恢复运行。 人工介入：通过在执行过程中的任何时刻检查和修改智能体状态，无缝融入人工监督。 全面记忆：创建真正有状态的智能体，具备用于持续推理的短期工作记忆和跨会话的长期持久记忆。 使用LangSmith进行调试：借助可视化工具深入了解复杂的智能体行为，这些工具可追踪执行路径、捕捉状态转换并提供详细的运行时指标。 可投入生产的部署：利用专为应对有状态、长时间运行的工作流程所面临的独特挑战而设计的可扩展基础设施，自信地部署复杂的智能体系统。 初始化 # 安装：\npip install -U langgraph 然后，使用预构建组件创建一个智能体：\n# 安装必要库 # pip install dashscope langchain langchain-community langgraph from langgraph.prebuilt import create_react_agent from langchain_community.chat_models.tongyi import ChatTongyi from langchain_core.messages import HumanMessage from langchain.agents import Tool import os def get_weather(city: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;获取指定城市的天气信息\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;{city}的天气是晴天，25℃！\u0026#34; # 创建千问模型实例 model = ChatTongyi( model_name=\u0026#34;qwen-turbo\u0026#34;, # 也可以使用 qwen-plus 或 qwen-max dashscope_api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;) # 替换为真实的API密钥 ) # 创建代理 agent = create_react_agent( model=model, tools=[Tool( name=\u0026#34;get_weather\u0026#34;, func=get_weather, description=\u0026#34;获取城市的天气信息\u0026#34; )], prompt=\u0026#34;你是一个有用的天气助手\u0026#34; ) # 运行代理 response = agent.invoke({ \u0026#34;messages\u0026#34;: [ HumanMessage(content=\u0026#34;上海的天气怎么样？\u0026#34;) ] }) # 打印响应结果 print(\u0026#34;最终回答:\u0026#34;, response[\u0026#34;messages\u0026#34;][-1].content) 快速入门 # StateGraph # 首先stategraph是用来描述整个图的，图中的状态会随着多个agent的工作不断的更新，节点node就是用来更新状态的如何来定义一张图中的状态\nfrom typing import Annotated from typing_extensions import TypedDict from langgraph.graph import StateGraph, START from langgraph.graph.message import add_messages class State(TypedDict): # Messages have the type \u0026#34;list\u0026#34;. The `add_messages` function # in the annotation defines how this state key should be updated # (in this case, it appends messages to the list, rather than overwriting them) messages: Annotated[list, add_messages] graph_builder = StateGraph(State)#State必须。定义状态数据的 Pydantic 模型，描述图中的数据结构 我们的图现在可以处理两个关键任务：\n每个node都可以接收当前State作为输入，并输出对该状态的更新。 由于使用了带有Annotated语法的预构建add_messages函数，对messages的更新将追加到现有列表中，而不是覆盖它。 Nodes # Nodes 代表工作单元，通常是普通的 Python 函数。\n首先选择一个模型：\nfrom langchain_community.chat_models.tongyi import ChatTongyi llm = ChatTongyi( model_name=\u0026#34;qwen-turbo\u0026#34;, # 也可使用 qwen-plus 或 qwen-max dashscope_api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;) # 替换为真实的API密钥 ) 将聊天模型整合到一个简单的节点中：\ndef chatbot(state: State): return {\u0026#34;messages\u0026#34;: [llm.invoke(state[\u0026#34;messages\u0026#34;])]} # The first argument is the unique node name # The second argument is the function or object that will be called whenever # the node is used. graph_builder.add_node(\u0026#34;chatbot\u0026#34;, chatbot) chatbot 节点函数接收当前的 State（状态）作为输入，并返回一个字典。这个字典包含一个更新后的 messages 列表，该列表位于键 \u0026ldquo;messages\u0026rdquo; 之下。这就是所有 LangGraph 节点函数的基本模式。\nState（状态）中的 add_messages 函数会将大型语言模型（LLM）的响应消息附加到状态中已有的任何消息之后。\nentry # 添加一个entry点，以便每次运行图时告知图从何处开始工作：\ngraph_builder.add_edge(START, \u0026#34;chatbot\u0026#34;) Compile # 在运行图之前，我们需要对其进行编译。我们可以通过在图构建器上调用compile()来实现。这将创建一个CompiledGraph，我们可以在状态上调用它。\ngraph = graph_builder.compile() 可视化 # from IPython.display import Image, display try: display(Image(graph.get_graph().draw_mermaid_png())) except Exception: # This requires some extra dependencies and is optional pass 运行 # def stream_graph_updates(user_input: str): for event in graph.stream({\u0026#34;messages\u0026#34;: [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}]}): for value in event.values(): print(\u0026#34;Assistant:\u0026#34;, value[\u0026#34;messages\u0026#34;][-1].content) while True: try: user_input = input(\u0026#34;User: \u0026#34;) if user_input.lower() in [\u0026#34;quit\u0026#34;, \u0026#34;exit\u0026#34;, \u0026#34;q\u0026#34;]: print(\u0026#34;Goodbye!\u0026#34;) break stream_graph_updates(user_input) except: # fallback if input() is not available user_input = \u0026#34;What do you know about LangGraph?\u0026#34; print(\u0026#34;User: \u0026#34; + user_input) stream_graph_updates(user_input) break 流式输出： stream() 方法返回一个可迭代的 event 流，每个 event 代表系统响应的一部分（可能是逐字生成的文本）。通过 for event in ... 循环，可以逐次处理这些流式事件。\n添加工具 # 定义搜索工具 # from langchain.agents import AgentExecutor, Tool, ZeroShotAgent from langchain.chains import LLMChain from langchain.memory import ConversationBufferMemory from langchain_community.utilities import SerpAPIWrapper from langchain_openai import ChatOpenAI # 定义Tool # 需要定义环境变量 export GOOGLE_API_KEY=\u0026#34;\u0026#34;, 在网站上注册并生成API Key: https://serpapi.com/searches search = SerpAPIWrapper() tools = [ Tool( name=\u0026#34;Search\u0026#34;, func=search.run, description=\u0026#34;useful for when you need to answer questions about current events\u0026#34;, ) ] 定义语言模型 # from langchain_community.chat_models.tongyi import ChatTongyi from langchain_core.messages import HumanMessage from langchain.agents import Tool import os # 创建千问模型实例 llm = ChatTongyi( model_name=\u0026#34;qwen-turbo\u0026#34;, # 也可以使用 qwen-plus 或 qwen-max dashscope_api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;) # 替换为真实的API密钥 ) 定义图 # bind_tools：这使得大语言模型（LLM）知道如果它想使用搜索引擎，应该使用的正确JSON格式。\nfrom typing import Annotated from typing_extensions import TypedDict from langgraph.graph import StateGraph, START, END from langgraph.graph.message import add_messages class State(TypedDict): messages: Annotated[list, add_messages] graph_builder = StateGraph(State) # Modification: tell the LLM which tools it can call llm_with_tools = llm.bind_tools(tools) def chatbot(state: State): return {\u0026#34;messages\u0026#34;: [llm_with_tools.invoke(state[\u0026#34;messages\u0026#34;])]} graph_builder.add_node(\u0026#34;chatbot\u0026#34;, chatbot) 创建一个函数来运行工具 现在，如果工具被调用，创建一个函数来运行这些工具。具体做法是，将这些工具添加到一个名为BasicToolNode的新节点中，该节点会检查状态中最新的消息，如果消息中包含tool_calls，就会调用工具。这依赖于大语言模型（LLM）的tool_calling支持，Anthropic、OpenAI、谷歌Gemini以及其他一些大语言模型提供商都提供这种支持。\nimport json from langchain_core.messages import ToolMessage class BasicToolNode: \u0026#34;\u0026#34;\u0026#34;A node that runs the tools requested in the last AIMessage.\u0026#34;\u0026#34;\u0026#34; def __init__(self, tools: list) -\u0026gt; None: self.tools_by_name = {tool.name: tool for tool in tools} def __call__(self, inputs: dict): if messages := inputs.get(\u0026#34;messages\u0026#34;, []): message = messages[-1] else: raise ValueError(\u0026#34;No message found in input\u0026#34;) outputs = [] for tool_call in message.tool_calls: tool_result = self.tools_by_name[tool_call[\u0026#34;name\u0026#34;]].invoke( tool_call[\u0026#34;args\u0026#34;] ) outputs.append( ToolMessage( content=json.dumps(tool_result), name=tool_call[\u0026#34;name\u0026#34;], tool_call_id=tool_call[\u0026#34;id\u0026#34;], ) ) return {\u0026#34;messages\u0026#34;: outputs} tool_node = BasicToolNode(tools=tools) graph_builder.add_node(\u0026#34;tools\u0026#34;, tool_node) 也可以使用LangGraph预先构建好的 ToolNode\nfrom typing import Annotated from langchain_tavily import TavilySearch from langchain_core.messages import BaseMessage from typing_extensions import TypedDict from langgraph.graph import StateGraph, START, END from langgraph.graph.message import add_messages from langgraph.prebuilt import ToolNode, tools_condition class State(TypedDict): messages: Annotated[list, add_messages] graph_builder = StateGraph(State) tool = TavilySearch(max_results=2) tools = [tool] llm_with_tools = llm.bind_tools(tools) def chatbot(state: State): return {\u0026#34;messages\u0026#34;: [llm_with_tools.invoke(state[\u0026#34;messages\u0026#34;])]} graph_builder.add_node(\u0026#34;chatbot\u0026#34;, chatbot) tool_node = ToolNode(tools=[tool]) graph_builder.add_node(\u0026#34;tools\u0026#34;, tool_node) graph_builder.add_conditional_edges( \u0026#34;chatbot\u0026#34;, tools_condition, ) # Any time a tool is called, we return to the chatbot to decide the next step graph_builder.add_edge(\u0026#34;tools\u0026#34;, \u0026#34;chatbot\u0026#34;) graph_builder.add_edge(START, \u0026#34;chatbot\u0026#34;) graph = graph_builder.compile() 定义条件边 # 添加了工具节点后，现在你可以定义条件边了。\n边将控制流从一个节点导向下一个节点。条件边从单个节点出发，通常包含 “if” 语句，以便根据当前图状态导向不同的节点。这些函数接收当前图状态，并返回一个字符串或字符串列表，指示接下来要调用哪个（哪些）节点。\n接下来，定义一个名为 route_tools 的路由函数，用于检查聊天机器人输出中的工具调用。通过调用 add_conditional_edges 将此函数提供给图，这会告诉图，每当聊天机器人节点完成时，检查此函数以确定接下来的走向。\n该条件在存在工具调用时将导向工具，不存在时则导向 END。由于该条件可以返回 END，这次你无需显式设置 finish_point。\nadd_conditional_edges参数：\nsource：起始节点。当退出此节点时，将运行此条件边。 path：可调用对象，用于确定下一个节点或多个节点。如果未指定path_map，则该可调用对象应返回一个或多个节点名称。如果返回END，图将#停止执行。 path_map：可选的路径到节点名称的映射。如果省略，path返回的路径应直接为节点名称。 def route_tools( state: State, ): \u0026#34;\u0026#34;\u0026#34; Use in the conditional_edge to route to the ToolNode if the last message has tool calls. Otherwise, route to the end. \u0026#34;\u0026#34;\u0026#34; if isinstance(state, list): ai_message = state[-1] elif messages := state.get(\u0026#34;messages\u0026#34;, []): ai_message = messages[-1] else: raise ValueError(f\u0026#34;No messages found in input state to tool_edge: {state}\u0026#34;) if hasattr(ai_message, \u0026#34;tool_calls\u0026#34;) and len(ai_message.tool_calls) \u0026gt; 0: return \u0026#34;tools\u0026#34; return END # The `tools_condition` function returns \u0026#34;tools\u0026#34; if the chatbot asks to use a tool, and \u0026#34;END\u0026#34; if # it is fine directly responding. This conditional routing defines the main agent loop. #add_conditional_edges参数： #source：起始节点。当退出此节点时，将运行此条件边。 #path：可调用对象，用于确定下一个节点或多个节点。如果未指定path_map，则该可调用对象应返回一个或多个节点名称。如果返回END，图将#停止执行。 #path_map：可选的路径到节点名称的映射。如果省略，path返回的路径应直接为节点名称。 graph_builder.add_conditional_edges( \u0026#34;chatbot\u0026#34;, route_tools, # The following dictionary lets you tell the graph to interpret the condition\u0026#39;s outputs as a specific node # It defaults to the identity function, but if you # want to use a node named something else apart from \u0026#34;tools\u0026#34;, # You can update the value of the dictionary to something else # e.g., \u0026#34;tools\u0026#34;: \u0026#34;my_tools\u0026#34; {\u0026#34;tools\u0026#34;: \u0026#34;tools\u0026#34;, END: END}, ) # Any time a tool is called, we return to the chatbot to decide the next step graph_builder.add_edge(\u0026#34;tools\u0026#34;, \u0026#34;chatbot\u0026#34;) graph_builder.add_edge(START, \u0026#34;chatbot\u0026#34;) graph = graph_builder.compile() 得到以下模型\n运行 # def stream_graph_updates(user_input: str): for event in graph.stream({\u0026#34;messages\u0026#34;: [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}]}): for value in event.values(): print(\u0026#34;Assistant:\u0026#34;, value[\u0026#34;messages\u0026#34;][-1].content) while True: try: user_input = input(\u0026#34;User: \u0026#34;) if user_input.lower() in [\u0026#34;quit\u0026#34;, \u0026#34;exit\u0026#34;, \u0026#34;q\u0026#34;]: print(\u0026#34;Goodbye!\u0026#34;) break stream_graph_updates(user_input) except: # fallback if input() is not available user_input = \u0026#34;What do you know about LangGraph?\u0026#34; print(\u0026#34;User: \u0026#34; + user_input) stream_graph_updates(user_input) break 添加记忆 # LangGraph 通过持久化检查点保存上下文信息。如果你在编译图时提供一个检查点工具，并在调用图时提供一个线程 ID，LangGraph 会在每一步之后自动保存状态。当你使用相同的线程 ID 再次调用图时，图会加载其保存的状态，使聊天机器人能够从上次中断的地方继续。\n创建MemorySaver检查点 # 创建基于内存的检查点\nfrom langgraph.checkpoint.memory import MemorySaver memory = MemorySaver() 编译图 # graph = graph_builder.compile(checkpointer=memory) 运行 # 选择一个线程作为此次对话的key：\nconfig = {\u0026#34;configurable\u0026#34;: {\u0026#34;thread_id\u0026#34;: \u0026#34;1\u0026#34;}} 运行图：\nuser_input = \u0026#34;Hi there! My name is Will.\u0026#34; # The config is the **second positional argument** to stream() or invoke()! events = graph.stream( {\u0026#34;messages\u0026#34;: [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}]}, config, stream_mode=\u0026#34;values\u0026#34;, ) for event in events: event[\u0026#34;messages\u0026#34;][-1].pretty_print() 继续询问：\nuser_input = \u0026#34;Remember my name?\u0026#34; # The config is the **second positional argument** to stream() or invoke()! events = graph.stream( {\u0026#34;messages\u0026#34;: [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}]}, config, stream_mode=\u0026#34;values\u0026#34;, ) for event in events: event[\u0026#34;messages\u0026#34;][-1].pretty_print() 检查状态 # 快照包含当前状态值、相应的配置以及next要处理的节点。\nsnapshot = graph.get_state(config) snapshot 如果在图形调用中获取状态，snapshot.next 会告知下一个将执行的节点\n添加人工介入控制 # 智能体可能不可靠，可能需要人类输入才能成功完成任务。同样，对于某些操作，你可能希望在运行前获得人类批准，以确保一切按预期运行。 LangGraph的持久层支持人在回路工作流程，允许根据用户反馈暂停和恢复执行。此功能的主要接口是interrupt函数。在节点内部调用interrupt将暂停执行。通过传入一个Command，可以连同来自人类的新输入一起恢复执行。interrupt在使用上类似于Python的内置函数input()，但有一些注意事项。\n添加human_assistance工具 # 定义模型：\nfrom langchain_community.chat_models.tongyi import ChatTongyi # 替换为千问模型 llm = ChatTongyi( model_name=\u0026#34;qwen-turbo\u0026#34;, # 也可使用 qwen-plus 或 qwen-max dashscope_api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;) # 替换为真实的API密钥 ) 添加 human_assistance ：\nfrom typing import Annotated from langchain_core.tools import tool, Tool from typing_extensions import TypedDict from langgraph.checkpoint.memory import MemorySaver from langgraph.graph import StateGraph, START, END from langgraph.graph.message import add_messages from langgraph.prebuilt import ToolNode, tools_condition from langgraph.types import Command, interrupt class State(TypedDict): messages: Annotated[list, add_messages] from langchain_community.utilities import SerpAPIWrapper # 定义Tool # 需要定义环境变量 export GOOGLE_API_KEY=\u0026#34;\u0026#34;, 在网站上注册并生成API Key: https://serpapi.com/searches # search = SerpAPIWrapper() # tools = [ # Tool( # name=\u0026#34;Search\u0026#34;, # func=search.run, # description=\u0026#34;useful for when you need to answer questions about current events\u0026#34;, # ) # ] class State(TypedDict): messages: Annotated[list, add_messages] graph_builder = StateGraph(State) @tool def human_assistance(query: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Request assistance from a human.\u0026#34;\u0026#34;\u0026#34; human_response = interrupt({\u0026#34;query\u0026#34;: query}) return human_response[\u0026#34;data\u0026#34;] @tool def Search(query: str) -\u0026gt; str: \u0026#34;Perform information retrieval based on user queries.\u0026#34; tool = SerpAPIWrapper() return tool.run(query) tools = [Search, human_assistance] llm_with_tools = llm.bind_tools(tools) def chatbot(state: State): message = llm_with_tools.invoke(state[\u0026#34;messages\u0026#34;]) # Because we will be interrupting during tool execution, # we disable parallel tool calling to avoid repeating any # tool invocations when we resume. assert len(message.tool_calls) \u0026lt;= 1 return {\u0026#34;messages\u0026#34;: [message]} graph_builder.add_node(\u0026#34;chatbot\u0026#34;, chatbot) tool_node = ToolNode(tools=tools) graph_builder.add_node(\u0026#34;tools\u0026#34;, tool_node) graph_builder.add_conditional_edges( \u0026#34;chatbot\u0026#34;, tools_condition, ) graph_builder.add_edge(\u0026#34;tools\u0026#34;, \u0026#34;chatbot\u0026#34;) graph_builder.add_edge(START, \u0026#34;chatbot\u0026#34;) 编译图 # memory = MemorySaver() graph = graph_builder.compile(checkpointer=memory) 向聊天机器人提问 # 向聊天机器人提出一个能调用的uman_assistance工具的问题：\nuser_input = \u0026#34;I need some expert guidance for building an AI agent. Could you request assistance for me?\u0026#34; config = {\u0026#34;configurable\u0026#34;: {\u0026#34;thread_id\u0026#34;: \u0026#34;1\u0026#34;}} events = graph.stream( {\u0026#34;messages\u0026#34;: [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}]}, config, stream_mode=\u0026#34;values\u0026#34;, ) for event in events: if \u0026#34;messages\u0026#34; in event: event[\u0026#34;messages\u0026#34;][-1].pretty_print() 输出：\n================================\u001b[1m Human Message \u001b[0m================================= I need some expert guidance for building an AI agent. Could you request assistance for me? ==================================\u001b[1m Ai Message \u001b[0m================================== Tool Calls: human_assistance (call_8fded5dd235844db9ea6eb) Call ID: call_8fded5dd235844db9ea6eb Args: query: I need some expert guidance for building an AI agent. 聊天机器人生成了一个工具调用，但随后执行被中断。如果你检查图状态，会发现它在工具节点处停止：\nsnapshot = graph.get_state(config) snapshot.next 与Python的内置input()函数类似，在工具内部调用interrupt将暂停执行。进度会根据checkpointer进行持久化；因此，如果使用Postgres进行持久化，只要数据库处于运行状态，就可以随时恢复执行。在此示例中，它使用内存中的checkpointer进行持久化，只要Python内核正在运行，就可以随时恢复。\n恢复执行 # 要恢复执行，请传递一个包含工具所需数据的Command对象。此数据的格式可根据需要自定义。在本示例中，使用一个键为\u0026quot;data\u0026quot;的字典：\nhuman_response = ( \u0026#34;We, the experts are here to help! We\u0026#39;d recommend you check out LangGraph to build your agent.\u0026#34; \u0026#34; It\u0026#39;s much more reliable and extensible than simple autonomous agents.\u0026#34; ) human_command = Command(resume={\u0026#34;data\u0026#34;: human_response}) events = graph.stream(human_command, config, stream_mode=\u0026#34;values\u0026#34;) for event in events: if \u0026#34;messages\u0026#34; in event: event[\u0026#34;messages\u0026#34;][-1].pretty_print() 自定义状态 # 在本教程中，将向状态添加更多字段，以便在不依赖消息列表的情况下定义复杂行为。聊天机器人将使用其搜索工具查找特定信息，并将其转发给人工进行审核。\n向状态添加键 # 通过向状态添加name和birthday键，更新聊天机器人以查询实体的生日：\nfrom typing import Annotated from typing_extensions import TypedDict from langgraph.graph.message import add_messages class State(TypedDict): messages: Annotated[list, add_messages] name: str birthday: str 更新工具内部的状态 # 在human_assistance工具内部填充状态键。这使得人类可以在信息存储到状态中之前对其进行审查。使用Command 从工具内部发出状态更新。\nfrom typing import Annotated from langchain_core.messages import ToolMessage from langchain_core.tools import InjectedToolCallId, tool from langgraph.types import Command @tool def human_assistance( name: str, birthday: str, tool_call_id: Annotated[str, InjectedToolCallId] ) -\u0026gt; Command: \u0026#34;\u0026#34;\u0026#34;Request assistance from a human.\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;--- Entering human_assistance tool ---\u0026#34;) print(f\u0026#34;Initial name: {name}, birthday: {birthday}\u0026#34;) from langgraph.types import interrupt human_response = interrupt( { \u0026#34;question\u0026#34;: \u0026#34;Is this correct?\u0026#34;, \u0026#34;name\u0026#34;: name, \u0026#34;birthday\u0026#34;: birthday, }, ) print(f\u0026#34;Human response: {human_response}\u0026#34;) if human_response.get(\u0026#34;correct\u0026#34;, \u0026#34;\u0026#34;).lower().startswith(\u0026#34;y\u0026#34;): verified_name = name verified_birthday = birthday response = \u0026#34;Correct\u0026#34; print(\u0026#34;Human confirmed correctness.\u0026#34;) else: verified_name = human_response.get(\u0026#34;name\u0026#34;, name) verified_birthday = human_response.get(\u0026#34;birthday\u0026#34;, birthday) response = f\u0026#34;Made a correction: {human_response}\u0026#34; print(f\u0026#34;Human made corrections. New name: {verified_name}, new birthday: {verified_birthday}\u0026#34;) state_update = { \u0026#34;name\u0026#34;: verified_name, \u0026#34;birthday\u0026#34;: verified_birthday, \u0026#34;messages\u0026#34;: [ToolMessage(response, tool_call_id=tool_call_id)], } print(f\u0026#34;State update being returned: {state_update}\u0026#34;) print(f\u0026#34;--- Exiting human_assistance tool ---\u0026#34;) return Command(update=state_update) 使用聊天机器人查询 # 提示聊天机器人查找LangGraph库的“诞生日期”，并指示聊天机器人在获取所需信息后调用human_assistance工具。通过在工具的参数中设置name和birthday，你可以迫使聊天机器人为这些字段生成建议。\nuser_input = ( \u0026#34;Can you look up when LangGraph was released? \u0026#34; \u0026#34;When you have the answer, use the human_assistance tool for review.\u0026#34; ) config = {\u0026#34;configurable\u0026#34;: {\u0026#34;thread_id\u0026#34;: \u0026#34;1\u0026#34;}} events = graph.stream( {\u0026#34;messages\u0026#34;: [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}]}, config, stream_mode=\u0026#34;values\u0026#34;, ) for event in events: if \u0026#34;messages\u0026#34; in event: event[\u0026#34;messages\u0026#34;][-1].pretty_print() 输出：\n================================\u001b[1m Human Message \u001b[0m================================= Can you look up when LangGraph was released? When you have the answer, use the human_assistance tool for review. ==================================\u001b[1m Ai Message \u001b[0m================================== Tool Calls: tavily_search (call_103316aa46a54bab99e235) Call ID: call_103316aa46a54bab99e235 Args: query: When was LangGraph released? =================================\u001b[1m Tool Message \u001b[0m================================= Name: tavily_search {\u0026#34;query\u0026#34;: \u0026#34;When was LangGraph released?\u0026#34;, \u0026#34;follow_up_questions\u0026#34;: null, \u0026#34;answer\u0026#34;: null, \u0026#34;images\u0026#34;: [], \u0026#34;results\u0026#34;: [{\u0026#34;title\u0026#34;: \u0026#34;Releases · langchain-ai/langgraph - GitHub\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://github.com/langchain-ai/langgraph/releases\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Releases · langchain-ai/langgraph · GitHub * fix(langgraph): remove deprecated `output` usage in favor of `output_schema` (#5095) * Remove Checkpoint.writes (#4822) * Remove old checkpoint test fixtures (#4814) * fix(langgraph): remove deprecated `output` usage in favor of `output_schema` (#5095) * Remove support for node reading a single managed value * Remove Checkpoint.writes (#4822) * Remove Checkpoint.pending_sends (#4820) * Remove old checkpoint test fixtures (#4814) Changes since checkpoint==2.0.26 * langgraph-checkpoint 2.1.0 * Preparation for 0.5 release: langgraph-checkpoint (#5124) * Remove Checkpoint.writes * Remove Checkpoint.pending_sends * Remove Checkpoint.writes (#4822) * Remove Checkpoint.pending_sends (#4820) * Remove old checkpoint test fixtures (#4814) * Remove postgres shallow checkpointer (#4813) * Remove Checkpoint.writes * Remove Checkpoint.pending_sends * Remove old checkpoint test fixtures * Remove postgres shallow checkpointer\u0026#34;, \u0026#34;score\u0026#34;: 0.98562, \u0026#34;raw_content\u0026#34;: null}, {\u0026#34;title\u0026#34;: \u0026#34;LangGraph - LangChain\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://www.langchain.com/langgraph\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Design agent-driven user experiences with LangGraph Platform\u0026#39;s APIs. Quickly deploy and scale your application with infrastructure built for agents. LangGraph sets the foundation for how we can build and scale AI workloads — from conversational agents, complex task automation, to custom LLM-backed experiences that \u0026#39;just work\u0026#39;. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.” LangGraph sets the foundation for how we can build and scale AI workloads — from conversational agents, complex task automation, to custom LLM-backed experiences that \u0026#39;just work\u0026#39;. LangGraph Platform is a service for deploying and scaling LangGraph applications, with an opinionated API for building agent UXs, plus an integrated developer studio.\u0026#34;, \u0026#34;score\u0026#34;: 0.98391, \u0026#34;raw_content\u0026#34;: null}], \u0026#34;response_time\u0026#34;: 1.65} ==================================\u001b[1m Ai Message \u001b[0m================================== Tool Calls: human_assistance (call_0fb6aff5612246d9976f84) Call ID: call_0fb6aff5612246d9976f84 Args: name: LangGraph birthday: 2023-10-01 --- Entering human_assistance tool --- Initial name: LangGraph, birthday: 2023-10-01 添加人工介入 # 聊天机器人未能识别出正确日期，因此请为其提供相关信息：\nhuman_command = Command( resume={ \u0026#34;name\u0026#34;: \u0026#34;LangGraph\u0026#34;, \u0026#34;birthday\u0026#34;: \u0026#34;Jan 17, 2024\u0026#34;, }, ) events = graph.stream(human_command, config, stream_mode=\u0026#34;values\u0026#34;) for event in events: if \u0026#34;messages\u0026#34; in event: event[\u0026#34;messages\u0026#34;][-1].pretty_print() 输出：\n==================================\u001b[1m Ai Message \u001b[0m================================== Tool Calls: human_assistance (call_0fb6aff5612246d9976f84) Call ID: call_0fb6aff5612246d9976f84 Args: name: LangGraph birthday: 2023-10-01 --- Entering human_assistance tool --- Initial name: LangGraph, birthday: 2023-10-01 Human response: {\u0026#39;name\u0026#39;: \u0026#39;LangGraph\u0026#39;, \u0026#39;birthday\u0026#39;: \u0026#39;Jan 17, 2024\u0026#39;} Human made corrections. New name: LangGraph, new birthday: Jan 17, 2024 State update being returned: {\u0026#39;name\u0026#39;: \u0026#39;LangGraph\u0026#39;, \u0026#39;birthday\u0026#39;: \u0026#39;Jan 17, 2024\u0026#39;, \u0026#39;messages\u0026#39;: [ToolMessage(content=\u0026#34;Made a correction: {\u0026#39;name\u0026#39;: \u0026#39;LangGraph\u0026#39;, \u0026#39;birthday\u0026#39;: \u0026#39;Jan 17, 2024\u0026#39;}\u0026#34;, tool_call_id=\u0026#39;call_0fb6aff5612246d9976f84\u0026#39;)]} --- Exiting human_assistance tool --- =================================\u001b[1m Tool Message \u001b[0m================================= Name: human_assistance Made a correction: {\u0026#39;name\u0026#39;: \u0026#39;LangGraph\u0026#39;, \u0026#39;birthday\u0026#39;: \u0026#39;Jan 17, 2024\u0026#39;} ==================================\u001b[1m Ai Message \u001b[0m================================== The release date of LangGraph is January 17, 2024. Let me know if you need any further assistance! 时间回溯 # 在典型的聊天机器人工作流程中，用户与机器人进行一次或多次交互以完成任务。记忆和人工介入可在图状态中设置检查点并控制未来的回复。 如果希望用户能够从先前的回复开始并探索不同的结果，或者，希望用户能够回溯聊天机器人的工作以纠正错误或尝试不同的策略可以使用LangGraph的内置时间回溯功能来创建此类功能。\n回退Graph # 通过使用图表的 get_state_history 方法获取检查点来回退graph。然后，你可以在这个先前的时间点恢复执行。\n","date":"20 June 2025","externalUrl":null,"permalink":"/byteglow/posts/langgraph/","section":"Posts","summary":"","title":"LangGraph","type":"posts"},{"content":"","date":"20 June 2025","externalUrl":null,"permalink":"/byteglow/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"20 June 2025","externalUrl":null,"permalink":"/byteglow/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"20 June 2025","externalUrl":null,"permalink":"/byteglow/series/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","section":"Series","summary":"","title":"学习笔记","type":"series"},{"content":" LlamaIndex # 简介 # LlamaIndex（以前称为 GPT Index）是一个用于构建基于文档的大语言模型（LLM）应用的开源工具。它通过提供灵活的索引结构和简单的接口，帮助开发者将外部数据源（如文档、数据库或API）与大语言模型相结合，从而更高效地查询和分析数据。以下是 LlamaIndex 的核心概念和功能简介：\n核心功能：\n数据整合与管理 多种数据源支持：支持导入多种数据格式，包括文本文件、PDF、CSV、SQL数据库等。 数据预处理：能够自动分块长文本、清理无关内容，并为后续的模型查询做好准备。 灵活的索引结构 提供了多种索引类型，例如树状索引（Tree Index）、列表索引（List Index）和图索引（Graph Index）。 开发者可以根据应用场景选择合适的索引，以优化查询效率或结果质量。 智能查询引擎 基于构建的索引，可以直接使用 LLM 进行语义搜索和自然语言问答。 支持自定义查询模板，以适配不同领域的需求。 与主流 LLM 的集成 支持与 OpenAI、Anthropic、Cohere 等多种主流语言模型的无缝集成。 可以通过配置文件快速切换不同模型。 1. LlamaIndex 使用模板 # LlamaIndex 常用使用模版：\n读取文档 (手动添加or通过Loader自动添加)；\n将文档解析为Nodes；\n构建索引（从文档or从Nodes，如果从文档，则对应函数内部会完成第2步的Node解析）\n[可选，进阶] 在其他索引上构建索引，即多级索引结构\n查询索引并对话大模型\nLlamaIndex的核心是将文档分解为多个Node对象。节点是LlamaIndex中的一等公民。节点表示源文档的“块”，无论是文本块、图像块还是更多。它们还包含元数据以及与其他节点和索引结构的关系信息。当您创建索引时，它抽象了节点的创建，但是，如果您的需求需要，您可以手动为文档定义节点。\n安装：\n!pip install llama-index !pip install llama-index-core !pip install llama-index-llms-dashscope !pip install llama-index-embeddings-dashscope 1.1 读取文档 # 使用data loaders读取 from llama_index import SimpleDirectoryReader # 从文件夹读取 documents = SimpleDirectoryReader(input_dir=\u0026#39;./data\u0026#39;).load_data() # 从指定文件读取，输入为List documents = SimpleDirectoryReader(input_files=[\u0026#39;./data/file.txt\u0026#39;]).load_data() 或者直接把自己的text改为document文档 from llama_index import Document # 直接从文本转换 text_list = [text1, text2, ...] documents = [Document(t) for t in text_list] 1.2 将文档解析为Nodes # 文档是轻量化的数据源容器，可以将文档：\n解析为 Node 对象\nfrom llama_index.node_parser import SimpleNodeParser parser = SimpleNodeParser() nodes = parser.get_nodes_from_documents(documents) 直接喂入 Index，函数内部会完成转化Node过程\nfrom llama_index import GPTSimpleVectorIndex index = GPTSimpleVectorIndex.from_documents(documents) 或者从Node构建Index\nfrom llama_index import GPTSimpleVectorIndex index = GPTSimpleVectorIndex(nodes) 1.3 构建索引 # 详见[使用LlamaIndex索引](#使用 LlamaIndex 索引)\n当想在多个索引中，复用一个 Node 时，可以通过定义 DocumentStore 结构，并在添加Nodes时指定 DocumentStore\nfrom gpt_index.docstore import SimpleDocumentStore docstore = SimpleDocumentStore() docstore.add_documents(nodes) index1 = GPTSimpleVectorIndex(nodes, docstore=docstore) index2 = GPTListIndex(nodes, docstore=docstore) 也可以将文档插入到索引\nfrom llama_index import GPTSimpleVectorIndex index = GPTSimpleVectorIndex([]) for doc in documents: index.insert(doc) 存储 Index 下次用\nimport os.path as osp index_file = \u0026#34;data/indices/index.json\u0026#34; if not osp.isfile(index_file): # 判断是否存在，不存在则创建 index = GPTSimpleVectorIndex.from_documents(documents) index.save_to_disk(index_file, encoding=\u0026#39;utf-8\u0026#39;) else: # 存在则 load index = GPTSimpleVectorIndex.load_from_disk(index_file) 1.4 查询索引并对话大模型 # 在默认情况下，查询索引通常采用问答形式，无需指定额外参数：\nPython\n# 简单查询 response = index.query(\u0026#34;作者成长过程中做了什么？\u0026#34;) print(response) response = index.query(\u0026#34;请根据用户的背景信息撰写一封邮件。\u0026#34;) print(response) 您也可以根据所使用的索引类型，通过添加额外参数来优化查询。\n设置模式（mode） mode 参数允许您指定底层模型的行为。以 ListIndex 为例，主要有两种选项：\ndefault: 此模式采用“创建并完善”的方法，顺序遍历每个 Node 以构建答案。 embedding: 此模式根据与查询最相似的 top-k 个节点（通过嵌入向量确定）来合成回复。 index = GPTListIndex.from_documents(documents) # 使用默认模式 response = index.query(\u0026#34;作者成长过程中做了什么？\u0026#34;, mode=\u0026#34;default\u0026#34;) # 使用嵌入模式 response = index.query(\u0026#34;作者成长过程中做了什么？\u0026#34;, mode=\u0026#34;embedding\u0026#34;) 设置回复模式（response_mode） 注意： 此选项不适用于 GPTTreeIndex。\nresponse_mode 参数影响答案是如何从检索到的节点中构建的：\ndefault: 对于给定的索引，此模式通过顺序浏览每个节点来“创建并完善”答案。每个节点都涉及一次独立的 LLM 调用。这对于获取更详细的答案很有益。 compact: 在每次 LLM 调用过程中，此模式通过填充尽可能多的节点文本块（以适应最大提示大小）来“紧凑”提示。如果一个提示中塞满了太多块，则通过多个提示来“创建并完善”答案。 tree_summarize: 给定一组节点和查询，此模式递归地构建一棵树并将根节点作为响应返回。这对于摘要目的很有用。 index = GPTListIndex.from_documents(documents) # 默认回复模式 response = index.query(\u0026#34;作者成长过程中做了什么？\u0026#34;, response_mode=\u0026#34;default\u0026#34;) # 紧凑回复模式 response = index.query(\u0026#34;作者成长过程中做了什么？\u0026#34;, response_mode=\u0026#34;compact\u0026#34;) # 树状总结回复模式 response = index.query(\u0026#34;作者成长过程中做了什么？\u0026#34;, response_mode=\u0026#34;tree_summarize\u0026#34;) 设置 required_keywords 和 exclude_keywords 您可以在大多数索引上设置 required_keywords 和 exclude_keywords（GPTTreeIndex 除外）。这可以预先过滤掉不包含 required_keywords 或包含 exclude_keywords 的节点，从而减少搜索空间，进而减少 LLM 调用/成本的时间/数量。\nPython\nresponse = index.query( \u0026#34;作者在 Y Combinator 之后做了什么？\u0026#34;, required_keywords=[\u0026#34;Combinator\u0026#34;], exclude_keywords=[\u0026#34;Italy\u0026#34;] ) 解析回复 查询的回复通常包含答案文本和用于生成回复的来源节点。\nPython\nresponse = index.query(\u0026#34;\u0026lt;query_str\u0026gt;\u0026#34;) # 获取回复文本 print(response.response) # 注意：根据库版本，通常直接通过 .response 或 .response_str 访问 # 获取来源节点 source_nodes = response.source_nodes print(source_nodes) # 获取格式化的来源（如果可用） formatted_sources = response.get_formatted_sources() print(formatted_sources) 2. 使用 LlamaIndex 索引 # 设置阿里dash scope 模型：\nfrom llama_index.core import Settings from llama_index.llms.dashscope import DashScope from llama_index.embeddings.dashscope import DashScopeEmbedding # \u0026lt;--- Changed this line import os os.environ[\u0026#34;DASHSCOPE_API_KEY\u0026#34;]=\u0026#34;\u0026lt;your-api-key\u0026gt;\u0026#34; os.environ[\u0026#34;ALIYUN_BASE_URL\u0026#34;]=\u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#34; #设置 llm Settings.llm = DashScope( # \u0026lt;--- Changed this line api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;), model=\u0026#34;qwen-plus\u0026#34; ) # 配置嵌入模型 ⭐ 新增设置 Settings.embed_model = DashScopeEmbedding( model_name=\u0026#34;text-embedding-v3\u0026#34;, # 百炼嵌入模型 api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;), ) 加载华为2024年度财报PDF文档：\nimport logging import sys ## showing logs logging.basicConfig(stream=sys.stdout, level=logging.INFO) logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout)) ## load the PDF from langchain.text_splitter import RecursiveCharacterTextSplitter from llama_index.core import download_loader # define loader UnstructuredReader = download_loader(\u0026#39;UnstructuredReader\u0026#39;, refresh_cache=True) loader = UnstructuredReader() # load the data documents = loader.load_data(\u0026#39;data/annual_report_2024_cn.pdf\u0026#39;,split_documents=False) 2.1 List Index # 列表索引是一种简单的数据结构，其中节点按顺序存储。在索引构建期间，文档文本被分块、转换为节点并存储在列表中。\n索引构建流程：\n文档切分：原始文档被分割成更小的文本块（如按段落、固定长度）。\n点生成：每个文本块转换为一个 Node 对象，生成对应的向量嵌入。\n列表存储：所有节点按顺序存入列表，形成索引结构。\n在查询期间，如果没有指定其他查询参数，LlamaIndex只是将列表中的所有node加载到Response Synthesis模块中。适用于需全局信息的查询（如摘要生成、全局分析）\n列表索引提供了许多查询列表索引的方法，如从基于嵌入的查询中获取前k个邻居，或者添加一个关键字过滤器，如下所示:\n创建 List Index并查询：\nfrom llama_index.core import GPTListIndex from IPython.display import Markdown, display # Example documents (replace with your own) # Build the index from documents index = GPTListIndex.from_documents(documents) # Create a query engine from the index query_engine = index.as_query_engine() # Query the index response = query_engine.query(\u0026#34;净收入为多少？\u0026#34;) display(Markdown(f\u0026#34;\u0026lt;b\u0026gt;{response}\u0026lt;/b\u0026gt;\u0026#34;)) # Alternative query engine setup with retriever_mode specified query_engine_with_embedding = index.as_query_engine( retriever_mode=\u0026#34;embedding\u0026#34;, verbose=True ) # Query with the alternative query engine response_with_embedding = query_engine_with_embedding.query(\u0026#34;净收入为多少?\u0026#34;) display(Markdown(f\u0026#34;\u0026lt;b\u0026gt;{response_with_embedding}\u0026lt;/b\u0026gt;\u0026#34;)) 输出：\n新提供的上下文中没有提及具体的净收入数值。因此，根据给定的信息，无法直接得出新的净收入数额。基于此，对于“净收入为多少？”这一问题的回答保持不变。 2024年，华为的净利润为人民币62,574百万元。请注意，您所询问的是净收入，而给出的数据是净利润，这两个概念虽然相关但并不相同。在当前提供的信息中，并没有直接提供关于净收入的具体数据。 2024年的净收入为861,335百万元人民币，2023年的净收入为703,246百万元人民币。这里提到的净收入指的是客户合同收入 LlamaIndex 能够为列表索引提供 Embedding 支持。\n# Alternative query engine setup with retriever_mode specified query_engine_with_embedding = index.as_query_engine( retriever_mode=\u0026#34;embedding\u0026#34;, verbose=True ) # Query with the alternative query engine response_with_embedding = query_engine_with_embedding.query(\u0026#34;What is net operating income?\u0026#34;) display(Markdown(f\u0026#34;\u0026lt;b\u0026gt;{response_with_embedding}\u0026lt;/b\u0026gt;\u0026#34;)) 2.2 向量存储索引 # 向量存储索引（Vector Store Index） 是 LlamaIndex 中的一种索引类型，基于嵌入向量实现高效的语义检索。通过将文本表示为向量并存储在专门的数据库中，查询时可以快速找到与查询语句语义最相关的内容。\n索引构建流程：\n文档分割：长文档被拆分为文本块（chunks），例如每块 500 字符。 节点生成：每个文本块转化为 Node 对象。 向量化：调用嵌入模型 API为每个 Node 生成向量。 向量存储：所有向量存入专用数据库。 查询流程：\n查询向量化：用户输入（如 \u0026ldquo;气候变化的影响\u0026rdquo;）通过相同的嵌入模型转换为向量。 相似度搜索：向量数据库计算查询向量与所有存储向量的相似度，返回 Top-K 最相似节点（如 K=5）。 响应合成：LLM（如 GPT-4）根据这些节点生成自然语言响应。 from llama_index.core import Document, GPTVectorStoreIndex # Example documents (replace with your actual data) documents = [ Document(text=\u0026#34;The author spent their childhood exploring forests, climbing trees, and reading books.\u0026#34;) ] # Build the index from the documents index = GPTVectorStoreIndex.from_documents(documents) # Create a query engine from the index query_engine = index.as_query_engine() # Query the index response = query_engine.query(\u0026#34;What did the author do growing up?\u0026#34;) print(response) 2.3 树状索引 # 树状索引是树结构索引，其中每个节点是子节点的摘要。在索引构建期间，树以自下而上的方式构建，直到我们最终得到一组根节点。\n树索引的构建过程是一个自下而上的分层聚合流程：\n叶节点层： 每个文本块被作为一个叶节点。 每个叶节点存储对应文本块及其嵌入向量。 中间节点层： 将叶节点按照一定数量（如 5 个或 10 个）分组。 对每组的文本块嵌入进行聚合（如取平均值或加权平均），生成一个代表这一组的嵌入向量。 为每组生成一个中间节点，存储代表性嵌入向量和摘要信息。 根节点： 对中间节点再次聚合，直到只剩一个根节点。 根节点的嵌入向量是整个索引的全局表示。 查询树状索引涉及从根节点向下遍历到叶节点。默认情况下(child_branch_factor=1 )，查询在给定父节点的情况下选择一个子节点。如果child_branch_factor=2，则查询在每个层级选择两个子节点。\n示例代码：\nfrom llama_index.core import GPTTreeIndex new_index = GPTTreeIndex.from_documents(documents) response = query_engine.query(\u0026#34;净收入为多少?\u0026#34;) display(Markdown(f\u0026#34;\u0026lt;b\u0026gt;{response}\u0026lt;/b\u0026gt;\u0026#34;)) ## if you want to have more content from the answer, # you can add the parameters child_branch_factor # let\u0026#39;s try using branching factor 2 query_engine = new_index.as_query_engine( child_branch_factor=2 ) response = query_engine.query(\u0026#34;净收入为多少?\u0026#34;) display(Markdown(f\u0026#34;\u0026lt;b\u0026gt;{response}\u0026lt;/b\u0026gt;\u0026#34;)) 在查询期间构建树状索引\n在 LlamaIndex 中，「查询时构建树状索引」是一种按需计算的优化技术，核心思想是延迟索引构建成本，直到真正需要时才根据查询动态创建树状结构。\n工作原理对比\n索引构建方式 构建时机 存储成本 响应延迟 适用场景 预先构建索引 数据加载时 高（完整树） 低（查询快） 高频查询的静态数据 查询时动态构建 首次查询时 低（仅叶节点） 高（需计算） 低频查询/动态数据 为此，可以设置以下参数：\nretriever_mode：指定检索模式，例如 \u0026ldquo;all_leaf\u0026rdquo; 表示使用所有叶节点。 response_mode：指定响应模式，例如 \u0026ldquo;tree_summarize\u0026rdquo; 表示以树状摘要模式生成响应。 在构建索引时，将 build_tree=False。 from llama_index import GPTKeywordTableIndex, Document # 1. 初始只创建叶节点层（不构建树） documents = [Document(text=\u0026#34;段落1...\u0026#34;), Document(text=\u0026#34;段落2...\u0026#34;)] index = GPTKeywordTableIndex.from_documents( documents, build_tree=False # 关键设置：仅生成叶节点 ) # 2. 查询时动态构建树并生成响应 query_engine = index.as_query_engine( retriever_mode=\u0026#34;all_leaf\u0026#34;, # 使用所有叶节点为基础 response_mode=\u0026#34;tree_summarize\u0026#34; # 树状摘要模式 ) # 首次查询触发树构建 response = query_engine.query(\u0026#34;气候变化的主要影响？\u0026#34;) 2.4 关键词索引 # 关键字表索引从每个Node提取关键字，并构建从每个关键字到该关键字对应的Node的映射。\n在查询时，我们从查询中提取相关关键字，并将其与预提取的Node关键字进行匹配，获取相应的Node。提取的节点被传递到响应合成模块。\n示例代码：\nfrom llama_index.core import GPTKeywordTableIndex index = GPTKeywordTableIndex.from_documents(documents) query_engine = index.as_query_engine() response = query_engine.query(\u0026#34;净收入为多少?\u0026#34;) 源码解析：\n调用llm 生成关键词： def _extract_keywords(self, text: str) -\u0026gt; Set[str]: \u0026#34;\u0026#34;\u0026#34;Extract keywords from text.\u0026#34;\u0026#34;\u0026#34; response = self._llm.predict( self.keyword_extract_template, text=text, ) return extract_keywords_given_response(response, start_token=\u0026#34;KEYWORDS:\u0026#34;) DEFAULT_KEYWORD_EXTRACT_TEMPLATE_TMPL = ( \u0026#34;Some text is provided below. Given the text, extract up to {max_keywords} \u0026#34; \u0026#34;keywords from the text. Avoid stopwords.\u0026#34; \u0026#34;---------------------\\n\u0026#34; \u0026#34;{text}\\n\u0026#34; \u0026#34;---------------------\\n\u0026#34; \u0026#34;Provide keywords in the following comma-separated format: \u0026#39;KEYWORDS: \u0026lt;keywords\u0026gt;\u0026#39;\\n\u0026#34; ) Max_keywords 默认 10\n关键字提取： def extract_keywords_given_response( response: str, lowercase: bool = True, start_token: str = \u0026#34;\u0026#34; ) -\u0026gt; Set[str]: \u0026#34;\u0026#34;\u0026#34; 在从给定的文本字符串中提取关键词应。 将\u0026lt;start_token\u0026gt;: \u0026lt;word1\u0026gt;, \u0026lt;word2\u0026gt;, ...解析为[word1, word2, ...] 如果响应未以\u0026lt;start_token\u0026gt;开头，则抛出异常 \u0026#34;\u0026#34;\u0026#34; ... # 如果关键字由多个单词组成拆分为子词 # (去除停用词) return expand_tokens_with_subtokens(set(results)) 2.5 可组合性图索引 # 可组合性图索引（Composable Graph Index）是 LlamaIndex 提供的高级索引结构，用于整合多个子索引的信息并进行跨文档查询。这种索引特别适用于需要对多个相关但独立的数据源进行联合分析的场景，比如：\n分析不同年份的财务报告 比较多篇学术论文的技术细节 整合多个知识库的专家系统 结合产品说明和用户反馈的分析系统 核心概念\n子索引（Subindexes）\n每个数据源建立独立的索引（如 VectorStoreIndex） 保留各数据源的上下文和特性 支持个性化配置（解析器、参数等） 组合索引（Composed Graph）\n将子索引整合成一个有向图 添加元信息描述各子索引内容 实现跨索引的信息路由和综合 索引构建流程：\n初始化独立子索引 子索引关系映射为每个子索引创建 IndexNode 描述符 构建索引图结构 查询过程：\n初始查询与路由 当用户提交一个查询请求时，这个请求首先会到达可组合性图索引的顶层节点。这个顶层节点通常是一个路由器（Router）。\n路由器（Router）的作用： 路由器是可组合性图索引的核心，它负责根据用户的查询意图，智能地决定将查询转发给哪个（或哪些）子索引进行处理。这就像一个智能的总机，根据你的需求把你转接到相应的部门。\n路由决策： 路由器通常会包含一些元数据或描述，用于解释每个子索引包含什么类型的信息。LlamaIndex 会利用语言模型的能力，分析用户的查询，并将其与这些元数据进行匹配，从而选择最相关的子索引。\n子索引查询与执行 一旦路由器确定了目标子索引，查询就会被转发到该子索引。\n并行或顺序执行：\n并行： 如果用户的查询可能涉及到多个子索引（例如，“查找所有关于人工智能和机器学习的信息”），路由器可能会将查询并行地发送给多个相关的子索引。 顺序： 在某些情况下，查询可能需要按顺序通过多个子索引。例如，你可能先在一个 SQL 索引中查询出某些 ID，然后将这些 ID 传递给一个向量索引来查找相关的文档。 特定索引类型的处理： 每个子索引都会根据其自身的类型来处理查询。\n向量索引： 如果是向量索引，查询会被向量化，然后进行相似性搜索以检索最相关的文本块。 列表索引： 如果是列表索引，可能会遍历列表中的所有节点以找到匹配项。 树索引： 如果是树索引，查询会沿着树的路径进行遍历，或者对子节点进行递归查询。 Pandas/SQL 索引： 如果是 Pandas 或 SQL 索引，LlamaIndex 会尝试将自然语言查询转换为相应的 Pandas 代码或 SQL 语句，并在 DataFrame 或数据库上执行。 结果合并与后处理 当所有选定的子索引完成查询并返回结果后，这些结果会回到顶层节点。\n结果聚合： 如果查询涉及多个子索引，LlamaIndex 会将它们各自返回的结果进行聚合。这可能包括合并文本块、数据条目或其他相关信息。\n答案合成： 在聚合之后，LlamaIndex 会再次利用语言模型的能力，对这些原始结果进行综合、提炼和组织，最终生成一个连贯、有意义且符合用户查询的最终答案。这个过程可能包括：\n去重和过滤： 移除冗余或不相关的结果。 总结和概括： 对检索到的信息进行总结，提供高层次的洞察。 格式化： 将答案以易于理解的格式呈现给用户。 通过一个场景编写代码:我们将执行以下步骤来演示可组合性图索引的能力:\n加载多篇PDF格式的学术论文。 为每篇论文创建独立的“知识库”（索引）。 为每篇论文生成一个简短的摘要。 将所有论文的知识库组合成一个更大的、可相互关联的“总知识图谱”。 查询 from llama_index.core import Settings, StorageContext, VectorStoreIndex from llama_index.core.indices.composability import ComposableGraph from llama_index.llms.dashscope import DashScope from llama_index.embeddings.dashscope import DashScopeEmbedding from llama_index.readers.file import UnstructuredReader from llama_index.core.node_parser import SentenceSplitter import os # 设置API密钥和环境变量 os.environ[\u0026#34;DASHSCOPE_API_KEY\u0026#34;] = \u0026#34;\u0026lt;your-api-key\u0026gt;\u0026#34; # 配置模型设置 - 统一全局设置 Settings.llm = DashScope( api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;), model=\u0026#34;qwen-plus\u0026#34; ) # 关键修改1: 设置较小的批处理大小 Settings.embed_model = DashScopeEmbedding( model_name=\u0026#34;text-embedding-v3\u0026#34;, api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;), embed_batch_size=10 # 设置为安全值 (小于等于10) ) # 降低分块大小以加速处理（可选） Settings.node_parser = SentenceSplitter( chunk_size=384, # 减小分块大小以降低复杂度 chunk_overlap=15, include_prev_next_rel=True ) # 限制输出长度避免token超限（可选） Settings.num_output = 384 # 准备两篇论文的文档 paper_files = { \u0026#34;transformer\u0026#34;: \u0026#34;data/1706.03762v7.pdf\u0026#34;, # \u0026#34;Attention is All You Need\u0026#34; \u0026#34;vit\u0026#34;: \u0026#34;data/2010.11929v2.pdf\u0026#34; # \u0026#34;An Image is Worth 16x16 Words\u0026#34; } # 自定义PDF解析器 reader = UnstructuredReader() # 为每篇论文创建索引 index_set = {} doc_set = {} print(\u0026#34;开始创建单篇论文索引...\u0026#34;) for paper_name, file_path in paper_files.items(): print(f\u0026#34;正在处理: {paper_name} 论文...\u0026#34;) # 加载单篇论文 loader = SimpleDirectoryReader( input_files=[file_path], file_extractor={\u0026#34;.pdf\u0026#34;: reader} ) documents = loader.load_data() doc_set[paper_name] = documents # 创建存储上下文 storage_context = StorageContext.from_defaults() # 创建针对学术论文优化的索引 try: cur_index = VectorStoreIndex.from_documents( documents=documents, storage_context=storage_context, show_progress=True ) except Exception as e: print(f\u0026#34;索引创建错误，尝试减小批处理大小: {str(e)}\u0026#34;) # 如果还不行，尝试使用更小的批处理 from llama_index.core import ServiceContext service_context = ServiceContext.from_defaults(embed_batch_size=5) cur_index = VectorStoreIndex.from_documents( documents=documents, storage_context=storage_context, service_context=service_context, show_progress=True ) index_set[paper_name] = cur_index # 保存索引以备后用 storage_context.persist(f\u0026#39;./paper_indexes/{paper_name}\u0026#39;) print(f\u0026#34;已完成 {paper_name} 论文索引创建!\\n\u0026#34;) # 为每篇论文生成摘要 index_summaries = {} for paper_name in paper_files: print(f\u0026#34;正在为 {paper_name} 论文生成摘要...\u0026#34;) query_engine = index_set[paper_name].as_query_engine(similarity_top_k=3) # 减少检索量 # 使用较短提示减少响应长度（可选） response = query_engine.query( \u0026#34;总结这篇论文的核心贡献和技术创新点 (100字)\u0026#34; ) index_summaries[paper_name] = response.response print(f\u0026#34;{paper_name.upper()} 论文摘要:\\n{response.response}\\n\u0026#34;) # 创建组合索引（知识图谱） print(\u0026#34;正在创建组合索引...\u0026#34;) try: graph = ComposableGraph.from_indices( base_class=VectorStoreIndex, indices=[index_set[name] for name in paper_files], index_summaries=[index_summaries[name] for name in paper_files] ) except Exception as e: print(f\u0026#34;创建组合索引出错: {str(e)}\u0026#34;) # 如果出错，降级为简单索引 from llama_index.core import SummaryIndex all_nodes = [] for paper_name in paper_files: all_nodes.extend(index_set[paper_name].docstore.get_nodes(list(index_set[paper_name].index_struct.nodes_dict.keys()))) graph = SummaryIndex(all_nodes) # 保存组合索引 graph.storage_context.persist(f\u0026#39;./paper_indexes/combined\u0026#39;) print(\u0026#34;组合索引已保存!\\n\u0026#34;) # 配置查询引擎 custom_query_engines = { index_set[paper_name].index_id: index_set[paper_name].as_query_engine( similarity_top_k=3 # 减少子索引检索量 ) for paper_name in paper_files } # 确保最终查询批处理不超限（可选） query_engine = graph.as_query_engine( custom_query_engines=custom_query_engines, verbose=True, similarity_top_k=3 # 减少检索结果数量 ) # 执行跨论文查询 print(\u0026#34;执行跨论文比较查询...\u0026#34;) try: # 使用更短的查询（可选） response = query_engine.query( \u0026#34;比较Transformer和ViT的核心异同点:架构设计、注意力机制、数据处理方式\u0026#34; ) except Exception as e: print(f\u0026#34;查询出错: {str(e)}\u0026#34;) print(\u0026#34;尝试简化查询...\u0026#34;) response = query_engine.query( \u0026#34;比较Transformer和ViT的架构差异\u0026#34; ) print(\u0026#34;\\n跨论文查询结果:\u0026#34;) print(response.response) try: # 检查源节点存在性 print(\u0026#34;\\n响应来源:\u0026#34;) if hasattr(response, \u0026#39;source_nodes\u0026#39;) and response.source_nodes: for source_node in response.source_nodes: file_path = source_node.node.metadata.get(\u0026#39;file_path\u0026#39;, \u0026#39;未知来源\u0026#39;) preview = source_node.node.text[:40].replace(\u0026#39;\\n\u0026#39;, \u0026#39; \u0026#39;) + \u0026#34;...\u0026#34; print(f\u0026#34;- {preview} (来自 {file_path})\u0026#34;) else: print(\u0026#34;未找到来源信息\u0026#34;) except Exception as e: print(f\u0026#34;无法获取响应来源: {str(e)}\u0026#34;) 输出：\n开始创建单篇论文索引... 正在处理: transformer 论文... WARNING:root:\u0026#39;doc_id\u0026#39; is deprecated and \u0026#39;id_\u0026#39; will be used instead \u0026#39;doc_id\u0026#39; is deprecated and \u0026#39;id_\u0026#39; will be used instead \u0026#39;doc_id\u0026#39; is deprecated and \u0026#39;id_\u0026#39; will be used instead \u0026#39;doc_id\u0026#39; is deprecated and \u0026#39;id_\u0026#39; will be used instead Parsing nodes: 0%| | 0/1 [00:00\u0026lt;?, ?it/s] Generating embeddings: 0%| | 0/15 [00:00\u0026lt;?, ?it/s] 已完成 transformer 论文索引创建! 正在处理: vit 论文... WARNING:root:\u0026#39;doc_id\u0026#39; is deprecated and \u0026#39;id_\u0026#39; will be used instead \u0026#39;doc_id\u0026#39; is deprecated and \u0026#39;id_\u0026#39; will be used instead \u0026#39;doc_id\u0026#39; is deprecated and \u0026#39;id_\u0026#39; will be used instead \u0026#39;doc_id\u0026#39; is deprecated and \u0026#39;id_\u0026#39; will be used instead Parsing nodes: 0%| | 0/1 [00:00\u0026lt;?, ?it/s] Generating embeddings: 0%| | 0/29 [00:00\u0026lt;?, ?it/s] 已完成 vit 论文索引创建! 正在为 transformer 论文生成摘要... TRANSFORMER 论文摘要: 这篇论文提出了Transformer模型，完全基于注意力机制，摒弃了传统的循环和卷积结构。该模型在机器翻译任务上表现出色，不仅质量更优，而且具有更好的并行化能力，训练时间也大幅减少。此外，Transformer还成功应用于英语句法分析任务，展示了其良好的泛化性能。 正在为 vit 论文生成摘要... VIT 论文摘要: 该论文的核心贡献在于提出了视觉变换器（ViT），这是一种将自然语言处理中的变换器模型应用于计算机视觉任务的新方法。技术创新点包括使用自注意力机制来处理图像数据，以及在大规模数据集上进行预训练以提高模型性能。此外，论文还展示了ViT在多个基准测试中达到了与最先进的卷积神经网络相媲美的结果。 正在创建组合索引... 创建组合索引出错: ComposableGraph.from_indices() missing 2 required positional arguments: \u0026#39;root_index_cls\u0026#39; and \u0026#39;children_indices\u0026#39; 组合索引已保存! 执行跨论文比较查询... 跨论文查询结果: Transformer和ViT（Vision Transformer）在架构设计、注意力机制以及数据处理方式上有一些核心的异同点。 1. **架构设计**： - **相同点**：两者都基于Transformer架构，利用了自注意力机制来捕捉输入序列中的长距离依赖关系。它们都采用了编码器结构，并且每一层都包含了多头自注意力机制和前馈神经网络。 - **不同点**：原始的Transformer主要用于自然语言处理任务，如机器翻译，其输入是文本序列；而ViT则是将Transformer应用于计算机视觉领域，它首先将图像分割成一系列的patches，并将这些patches线性嵌入到一维序列中作为模型的输入。此外，ViT通常不包含解码器部分，而是直接使用编码器输出进行分类或其他视觉任务。 2. **注意力机制**： - **相同点**：两者都采用了多头自注意力机制，通过计算query, key, value之间的相似度来为每个位置分配权重，从而允许模型关注输入序列中最重要的部分。 - **不同点**：虽然基本原理一致，但具体实现细节可能有所不同。例如，在处理图像时，ViT需要考虑如何有效地将二维空间 响应来源: - 3 2 0 2 g u A 2 ] L C . s c [ 7 v 2 6... (来自 data/1706.03762v7.pdf) - Numerous efforts have since continued to... (来自 data/1706.03762v7.pdf) - Here, the encoder maps an input sequence... (来自 data/1706.03762v7.pdf) ... 源码解析：\n此方法创建一个 可组合的索引图，其中指定一个索引作为根节点 (root_index)，多个子索引 (children_indices) 作为叶节点。根索引负责将查询路由到相关的子索引。\ndef from_indices( cls, root_index_cls: Type[BaseIndex],#根索引的类 children_indices: Sequence[BaseIndex],#子索引列表 index_summaries: Optional[Sequence[str]] = None,#每个子索引的文本摘要 storage_context: Optional[StorageContext] = None, **kwargs: Any, ) -\u0026gt; \u0026#34;ComposableGraph\u0026#34;: # type: ignore \u0026#34;\u0026#34;\u0026#34;Create composable graph using this index class as the root.\u0026#34;\u0026#34;\u0026#34; from llama_index.core import Settings with Settings.callback_manager.as_trace(\u0026#34;graph_construction\u0026#34;): if index_summaries is None: for index in children_indices: if index.index_struct.summary is None: raise ValueError( \u0026#34;Summary must be set for children indices. \u0026#34; \u0026#34;If the index does a summary \u0026#34; \u0026#34;(through index.index_struct.summary), then \u0026#34; \u0026#34;it must be specified with then `index_summaries` \u0026#34; \u0026#34;argument in this function. We will support \u0026#34; \u0026#34;automatically setting the summary in the future.\u0026#34; ) index_summaries = [ index.index_struct.summary for index in children_indices ] else: # set summaries for each index for index, summary in zip(children_indices, index_summaries): index.index_struct.summary = summary if len(children_indices) != len(index_summaries): raise ValueError(\u0026#34;indices and index_summaries must have same length!\u0026#34;) # 遍历每个子索引和其对应的摘要，创建 IndexNode index_nodes = [] for index, summary in zip(children_indices, index_summaries): assert isinstance(index.index_struct, IndexStruct) index_node = IndexNode( text=summary, index_id=index.index_id, relationships={ NodeRelationship.SOURCE: RelatedNodeInfo( node_id=index.index_id, node_type=ObjectType.INDEX ) }, ) index_nodes.append(index_node) # 使用提供的 root_index_cls 类，将生成的 index_nodes 作为节点传入，构造根索引 root_index = root_index_cls( nodes=index_nodes, storage_context=storage_context, **kwargs, ) # 将所有子索引和根索引组合成一个字典，键为 index_id，值为对应的索引 all_indices: List[BaseIndex] = [ *cast(List[BaseIndex], children_indices), root_index, ] return cls( all_indices={index.index_id: index for index in all_indices}, root_id=root_index.index_id, storage_context=storage_context, ) 在 ComposableGraph 中，子索引和父索引之间的关系被抽象为一种有向图的关系结构。\n根索引（父索引） └── 子索引1：关于机器学习的知识 └── 子索引2：关于深度学习的知识 子索引与父索引之间的关系在代码中表现为一种有向连接：\n父索引通过 IndexNode 引用子索引。 子索引通过 NodeRelationship.SOURCE 链接回父索引。 这种关系形成了一个逻辑上的树状结构，支持数据的分层组织和高效检索。 2.6 文档摘要索引 # 这是一个全新的LlamaIndex数据结构，它是为了问答而制作的。\n通常，大多数用户以以下方式开发基于LLM的QA系统:\n获取源文档并将其分成文本块。 然后将文本块存储在矢量数据库中。 在查询期间，通过使用相似度和/或关键字过滤器进行Embedding来检索文本块。 执行整合后的响应。 然而，这种方法存在一些影响检索性能的局限性：\n文本块没有完整的全局上下文，这通常限制了问答过程的有效性。 需要仔细调优top-k /相似性分数阈值，因为过小的值可能会导致错过相关上下文，而过大的值可能会增加不相关上下文的成本和延迟。 Embeddings可能并不总是为一个问题选择最合适的上下文，因为这个过程本质上是分别决定文本和上下文的。 为了增强检索结果，一些开发人员添加了关键字过滤器。然而，这种方法有其自身的挑战，例如通过手工或使用NLP关键字提取/主题标记模型为每个文档确定适当的关键字，以及从查询中推断正确的关键字。\n这就是 LlamaIndex 引入文档摘要索引的原因，它可以为每份文档提取非结构化文本摘要并编制索引，从而提高检索性能，超越现有方法。该索引比单一文本块包含更多信息，比关键字标签具有更多语义。它还允许灵活的检索，包括基于 LLM 和嵌入的方法。在构建期间，该索引接收文档并使用 LLM 从每个文档中提取摘要。在查询时，它会根据摘要使用以下方法检索相关文档：\n基于 LLM 的检索：获取文档摘要集合并请求 LLM 识别相关文档+相关性得分 基于嵌入的检索：利用摘要嵌入相似性来检索相关文档，并对检索结果的数量施加顶k限制。 文档摘要索引的检索类为任何选定的文档检索所有节点，而不是在节点级返回相关块。\n索引构建\n文档加载与分块 生成摘要 向量化嵌入 构建索引 查询阶段\n用户输入查询\n查询向量化\n语义检索\n示例代码：\nfrom llama_index.core import ( Document, VectorStoreIndex, DocumentSummaryIndex, StorageContext, Settings, SimpleDirectoryReader, ServiceContext, ) from llama_index.core.schema import TextNode from llama_index.llms.dashscope import DashScope from llama_index.embeddings.dashscope import DashScopeEmbedding from llama_index.core.node_parser import SentenceSplitter from llama_index.core.extractors import SummaryExtractor from llama_index.core.ingestion import IngestionPipeline import os # 1. 配置环境 os.environ[\u0026#34;DASHSCOPE_API_KEY\u0026#34;] = \u0026#34;\u0026lt;your-api-key\u0026gt;\u0026#34; # Ensure this is correct and active # 全局设置 llm = DashScope(model=\u0026#34;qwen-plus\u0026#34;, api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;)) embed_model = DashScopeEmbedding( model_name=\u0026#34;text-embedding-v3\u0026#34;, api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;), embed_batch_size=8 ) # Set global LLM and Embed Model for LlamaIndex Settings.llm = llm Settings.embed_model = embed_model Settings.node_parser = SentenceSplitter( chunk_size=512, chunk_overlap=20, include_prev_next_rel=True ) # 2. 文档预处理 # --- MODIFIED: This function will now return Document objects --- def load_documents_from_files(file_paths: list): \u0026#34;\u0026#34;\u0026#34;加载文档并返回Document对象列表\u0026#34;\u0026#34;\u0026#34; all_documents = [] for file_path in file_paths: if not os.path.exists(file_path): print(f\u0026#34;文件不存在: {file_path}\u0026#34;) continue print(f\u0026#34;正在加载: {os.path.basename(file_path)}\u0026#34;) try: # SimpleDirectoryReader directly loads into Document objects docs = SimpleDirectoryReader( input_files=[file_path] ).load_data() # Add custom metadata to each document for doc in docs: doc.metadata[\u0026#34;source\u0026#34;] = os.path.basename(file_path) doc.metadata[\u0026#34;doc_type\u0026#34;] = \u0026#34;academic\u0026#34; all_documents.extend(docs) # Extend the list with new documents except Exception as e: print(f\u0026#34;文件 {os.path.basename(file_path)} 加载错误: {str(e)}\u0026#34;) print(f\u0026#34;已加载 {len(all_documents)} 个文档\u0026#34;) return all_documents # 3. 创建摘要索引管道 def create_summary_index(documents, storage_path): \u0026#34;\u0026#34;\u0026#34;创建文档摘要索引\u0026#34;\u0026#34;\u0026#34; if not documents: raise ValueError(\u0026#34;没有可用的文档\u0026#34;) # 创建摘要提取器 summary_extractor = SummaryExtractor( llm=llm, # Explicitly pass the LLM to the extractor summaries=[\u0026#34;self\u0026#34;], prompt=\u0026#34;生成此文档的技术摘要，突出核心贡献、创新点和关键技术\u0026#34;, ) # 创建解析管道 # --- MODIFIED: Add SentenceSplitter to the pipeline explicitly --- transformations = [ Settings.node_parser, # The global node parser (SentenceSplitter) will split documents into nodes summary_extractor # Then, SummaryExtractor will process these nodes ] # 创建处理管道 pipeline = IngestionPipeline( transformations=transformations, ) # 运行管道处理文档 # --- MODIFIED: Pass documents to the pipeline instead of pre-created nodes --- print(\u0026#34;正在通过管道处理文档并提取摘要...\u0026#34;) transformed_nodes = pipeline.run(documents=documents, show_progress=True) # Filter out any nodes that might somehow end up with None ref_doc_id (safety check) initial_node_count = len(transformed_nodes) transformed_nodes = [node for node in transformed_nodes if node.ref_doc_id is not None] if len(transformed_nodes) \u0026lt; initial_node_count: print(f\u0026#34;警告: {initial_node_count - len(transformed_nodes)} 个节点因 ref_doc_id 为 None 而被移除。\u0026#34;) # 创建存储上下文 storage_context = StorageContext.from_defaults() # 创建摘要索引 summary_index = DocumentSummaryIndex( nodes=transformed_nodes, # These nodes should now have proper ref_doc_id storage_context=storage_context, llm=llm, # Pass LLM to the index as well ) # 保存索引 os.makedirs(storage_path, exist_ok=True) # Ensure directory exists storage_context.persist(persist_dir=storage_path) print(f\u0026#34;摘要索引已保存到: {storage_path}\u0026#34;) return summary_index # 4. 创建查询引擎 (No change) def create_query_engine(index): \u0026#34;\u0026#34;\u0026#34;创建摘要查询引擎\u0026#34;\u0026#34;\u0026#34; return index.as_query_engine( response_mode=\u0026#34;tree_summarize\u0026#34;, ) # 5. 使用示例 if __name__ == \u0026#34;__main__\u0026#34;: # 文档路径配置 PAPER_PATHS = [ \u0026#34;data/academic/1706.03762v7.pdf\u0026#34;, # Transformer论文 \u0026#34;data/academic/2010.11929v2.pdf\u0026#34; # ViT论文 ] INDEX_PATH = \u0026#34;./paper_summary_index\u0026#34; # 步骤1: 加载文档 (不再创建TextNode，而是Document) documents = load_documents_from_files(PAPER_PATHS) if not documents: print(\u0026#34;没有可处理的文档，程序退出\u0026#34;) exit() # 步骤2: 创建摘要索引 # Pass the list of Document objects to the create_summary_index function summary_index = create_summary_index(documents, INDEX_PATH) # 步骤3: 创建查询引擎 query_engine = create_query_engine(summary_index) # 执行查询 queries = [ \u0026#34;总结Transformer的核心创新点\u0026#34;, \u0026#34;ViT是如何处理图像的?\u0026#34;, \u0026#34;Transformer和ViT在注意力机制应用上有何异同\u0026#34; ] for i, q in enumerate(queries): print(f\u0026#34;\\n{\u0026#39;=\u0026#39;*50}\\n查询 #{i+1}: {q}\\n{\u0026#39;-\u0026#39;*50}\u0026#34;) try: response = query_engine.query(q) print(f\u0026#34;结果:\\n{response.response}\\n\u0026#34;) except Exception as e: print(f\u0026#34;查询出错: {str(e)}\u0026#34;) 输出：\n正在加载: 1706.03762v7.pdf 正在加载: 2010.11929v2.pdf 已加载 37 个文档 正在通过管道处理文档并提取摘要... Parsing nodes: 0%| | 0/37 [00:00\u0026lt;?, ?it/s] 100%|██████████| 82/82 [04:52\u0026lt;00:00, 3.56s/it] 摘要索引已保存到: ./paper_summary_index ================================================== 查询 #1: 总结Transformer的核心创新点 -------------------------------------------------- 结果: Transformer的核心创新点主要包括以下几个方面： 1. **自注意力机制**：Transformer引入了多头自注意力机制，这使得模型能够并行处理输入序列，并且在处理长距离依赖关系时更加有效。 2. **编码器-解码器架构**：模型由编码器和解码器两部分组成，每部分都包含多个相同的层。这种结构允许模型有效地捕捉输入序列的信息，并生成相应的输出序列。 3. **位置前馈网络**：每个编码器和解码器层中都包含一个位置前馈网络，它对序列中的每个位置进行独立的线性变换，从而增强了模型的表达能力。 4. **残差连接与层归一化**：为了提高训练的稳定性和效率，Transformer在每个子层之后使用了残差连接和层归一化技术。 5. **位置编码**：由于Transformer没有递归或卷积结构，因此通过添加位置编码来为模型提供序列中元素的位置信息。 这些创新点共同使得Transformer在处理序列数据时具有高效、并行化和强大的表示能力。 ================================================== 查询 #2: ViT是如何处理图像的? -------------------------------------------------- 结果: ViT处理图像的过程包括以下几个步骤： 1. **图像分割**：首先，将输入的图像分割成固定大小的块（patches）。 2. **线性嵌入**：每个块被展平并进行线性投影，生成patch embeddings。这些嵌入向量形成一个序列。 3. **位置嵌入**：为了保留位置信息，向patch embeddings中添加可学习的位置嵌入。 4. **分类标记**：在patch embeddings序列的前面添加一个额外的可学习的“分类标记”（[class] token）。这个标记在通过Transformer编码器后，作为图像的表示用于分类任务。 5. **Transformer编码器**：将包含位置嵌入和分类标记的patch embeddings序列输入到标准的Transformer编码器中。编码器由多头自注意力机制（MSA）和多层感知机（MLP）交替组成，并且在每个块之前应用层归一化（LN），之后添加残差连接。 6. **分类头**：最后，将分类标记的输出通过一个分类头进行处理。在预训练阶段，分类头是一个具有一个隐藏层的多层感知机（MLP），而在微调阶段，它 ================================================== 查询 #3: Transformer和ViT在注意力机制应用上有何异同 -------------------------------------------------- 结果: Transformer 和 Vision Transformer (ViT) 在注意力机制的应用上有一些共同点和不同点。 **共同点：** - 两者都使用自注意力机制（self-attention）来处理输入数据。自注意力机制允许模型在处理每个元素时，能够关注到序列中的其他所有元素，从而捕捉长距离依赖关系。 - 自注意力机制的核心组成部分是查询（Query）、键（Key）和值（Value），这些组件在两种模型中都是通过线性变换从输入数据中生成的。 **不同点：** - **输入数据的处理方式：** - Transformer 主要用于处理一维序列数据，如文本。它将输入序列分割成固定长度的片段，并对每个片段应用自注意力机制。 - ViT 则专门设计用于处理二维图像数据。它将图像分割成多个小块（patches），并将每个块展平为一维向量，然后将其作为序列输入到自注意力层中。这样，ViT 能够直接应用于图像分类等任务。 - **位置编码：** - Transformer 使用位置编码来保留输入序列中元素的位置信息，因为自注意力机制本身不包含位置信息。 - ViT 同样需要 添加支持加载已生成的 index：\nif __name__ == \u0026#34;__main__\u0026#34;: PAPER_PATHS = [ \u0026#34;data/academic/1706.03762v7.pdf\u0026#34;, \u0026#34;data/academic/2010.11929v2.pdf\u0026#34; ] INDEX_PATH = \u0026#34;./paper_summary_index\u0026#34; summary_index = None # --- 添加加载逻辑 --- if os.path.exists(INDEX_PATH) and os.listdir(INDEX_PATH): print(f\u0026#34;检测到已存在的索引目录: {INDEX_PATH}，尝试加载索引...\u0026#34;) try: # 创建一个用于加载的 StorageContext storage_context = StorageContext.from_defaults(persist_dir=INDEX_PATH) # 使用 load_index_from_storage 加载索引 # 注意：对于DocumentSummaryIndex，你通常需要指定 type=DocumentSummaryIndex # 但如果它能自动推断，也可以省略。这里显式指定更安全。 summary_index = load_index_from_storage( storage_context=storage_context, llm=llm, # 加载时也传递LLM和Embed Model embed_model=embed_model, index_type=\u0026#34;document_summary\u0026#34; # 明确指定索引类型 ) print(\u0026#34;索引加载成功！\u0026#34;) except Exception as e: print(f\u0026#34;加载索引失败: {e}\u0026#34;) print(\u0026#34;将重新生成索引...\u0026#34;) # 如果加载失败，则回退到生成新索引 documents = load_documents_from_files(PAPER_PATHS) if not documents: print(\u0026#34;没有可处理的文档，程序退出\u0026#34;) exit() summary_index = create_summary_index(documents, INDEX_PATH) else: print(f\u0026#34;未检测到索引目录或目录为空: {INDEX_PATH}，正在生成新索引...\u0026#34;) documents = load_documents_from_files(PAPER_PATHS) if not documents: print(\u0026#34;没有可处理的文档，程序退出\u0026#34;) exit() summary_index = create_summary_index(documents, INDEX_PATH) if summary_index is None: print(\u0026#34;未能创建或加载索引，程序退出。\u0026#34;) exit() # 步骤3: 创建查询引擎 query_engine = create_query_engine(summary_index) # 执行查询 queries = [ \u0026#34;总结Transformer的核心创新点\u0026#34;, \u0026#34;ViT是如何处理图像的?\u0026#34;, \u0026#34;Transformer和ViT在注意力机制应用上有何异同\u0026#34; ] for i, q in enumerate(queries): print(f\u0026#34;\\n{\u0026#39;=\u0026#39;*50}\\n查询 #{i+1}: {q}\\n{\u0026#39;-\u0026#39;*50}\u0026#34;) try: response = query_engine.query(q) print(f\u0026#34;结果:\\n{response.response}\\n\u0026#34;) except Exception as e: print(f\u0026#34;查询出错: {str(e)}\u0026#34;) 特征 ref_doc_id node_id 层级 文档级（标识源文档） 节点级（标识单个数据单元） 生成方式 由文档 ingestion 自动生成（或手动指定） 由节点创建过程自动生成（或手动指定） 唯一性范围 全局唯一（每个文档对应一个 ref_doc_id） 全局唯一（每个节点对应一个 node_id） 典型操作 删除文档关联的所有节点（delete_ref_doc） 删除单个节点（delete_nodes） 使用场景 溯源、文档级管理 细粒度检索、节点关系建模 无文档场景 None（如 build_index_from_nodes） 必须存在（节点创建时强制生成） 2.7 知识图谱索引 # 它通过在一组文档中提取知识三元组（主语、谓语、宾语）来建立索引。 在查询时，它既可以只使用知识图谱作为上下文进行查询，也可以利用每个实体的底层文本作为上下文进行查询。通过利用底层文本，我们可以针对文档内容提出更复杂的查询。\n参考文档《高级 RAG》”使用知识图谱改进 RAG 检索“章节\n2.8 Pandas # 利用LlamaIndex的能力，通过其PandasQueryEngine查询Pandas数据帧。该引擎允许你针对dataframe提出自然语言问题，LlamaIndex会将这些问题转换为Pandas代码以获取答案。\nimport pandas as pd from llama_index.experimental.query_engine import PandasQueryEngine from llama_index.llms.openai import OpenAI # Or any other LLM you prefer from llama_index.llms.dashscope import DashScope from llama_index.core.extractors import SummaryExtractor from llama_index.core.ingestion import IngestionPipeline import os # 1. Create a sample Pandas DataFrame data = { \u0026#39;Name\u0026#39;: [\u0026#39;Alice\u0026#39;, \u0026#39;Bob\u0026#39;, \u0026#39;Charlie\u0026#39;, \u0026#39;David\u0026#39;], \u0026#39;Age\u0026#39;: [25, 30, 35, 40], \u0026#39;City\u0026#39;: [\u0026#39;New York\u0026#39;, \u0026#39;Los Angeles\u0026#39;, \u0026#39;Chicago\u0026#39;, \u0026#39;New York\u0026#39;], \u0026#39;Salary\u0026#39;: [70000, 80000, 90000, 75000] } df = pd.DataFrame(data) # 2. Initialize the LLM (e.g., OpenAI) # Make sure you have your OpenAI API key set as an environment variable # export OPENAI_API_KEY=\u0026#34;your_api_key_here\u0026#34; llm = DashScope(model=\u0026#34;qwen-plus\u0026#34;, api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;)) # 3. Create a PandasQueryEngine from your DataFrame query_engine = PandasQueryEngine(df=df, llm=llm, verbose=True) # 4. Query your DataFrame with natural language response = query_engine.query(\u0026#34;What is the average age of people from New York?\u0026#34;) print(response) response = query_engine.query(\u0026#34;Show me the names of people older than 30.\u0026#34;) print(response) 2.8 总结 # 这是一张关于不同索引类型（Type）的表格，包含以下内容：\n索引类型（Type） 用途（Purpose） 是否涉及大语言模型调用/嵌入费用（LLM Call aka Embeeding Fee） Vector Store Index 易于使用，可基于大规模数据语料库回答查询 是 List Index 有助于综合跨多个数据源信息来生成答案 否，仅在执行数据检索时涉及 Tree Index 有助于总结文档集合 否，仅在执行数据检索时涉及 Keyword Table Index 有助于将查询路由到不同数据源 是 Composability Graph Index 有助于构建知识图谱并堆叠多个索引 是 Document Summary Index 为问答目的预先构建 是 这些索引类型在处理数据、回答查询、构建知识图谱等不同场景中发挥作用，且在是否产生大语言模型相关调用或嵌入费用方面存在差异 。\n补充 # 1. Retriever_mode # LlamaIndex中不同索引类型对应的**检索器模式（Retriever Modes）**及其映射的检索器类。以下是分索引类型的详细解释：\n1. 向量索引（Vector Index）\n特点：检索器模式配置无效（会被忽略）。 默认行为：无论参数如何，调用 vector_index.as_retriever(...) 始终返回 VectorIndexRetriever，基于向量相似度进行检索。 2. 摘要索引（Summary Index）\n通过不同模式选择检索方式：\ndefault：使用 SummaryIndexRetriever，基于摘要内容直接检索。 embedding：使用 SummaryIndexEmbeddingRetriever，将摘要转为嵌入向量后检索（适合语义匹配）。 llm：使用 SummaryIndexLLMRetriever，通过LLM生成查询来检索（适合自然语言理解）。 3. 树索引（Tree Index）\n针对树结构数据的不同检索粒度：\nselect_leaf：TreeSelectLeafRetriever，检索匹配的叶子节点（适合细粒度查询）。 select_leaf_embedding：TreeSelectLeafEmbeddingRetriever，结合嵌入向量检索叶子节点。 all_leaf：TreeAllLeafRetriever，检索所有叶子节点（适合全面查询）。 root：TreeRootRetriever，直接检索根节点（适合获取整体摘要）。 4. 关键词表索引（Keyword Table Index）\n基于关键词提取方法选择检索器：\ndefault：KeywordTableGPTRetriever，使用GPT生成关键词检索（精度较高）。 simple：KeywordTableSimpleRetriever，基于简单关键词匹配（轻量级，速度快）。 rake：KeywordTableRAKERetriever，使用RAKE算法提取关键词（适合多词短语）。 5. 知识图谱索引（Knowledge Graph Index）\n不同模式对应知识图谱的查询方式：\nkeyword/embedding/hybrid：均使用 KGTableRetriever，分别通过关键词匹配、嵌入向量或混合方式检索知识图谱数据。 注：模式名称可能仅表示输入查询的类型，但检索器类不变。 6. 文档摘要索引（Document Summary Index）\n针对文档摘要的检索策略：\nllm：DocumentSummaryIndexLLMRetriever，通过LLM生成查询匹配摘要内容。 embedding：DocumentSummaryIndexEmbeddingRetrievers，基于摘要的嵌入向量检索 ","date":"20 June 2025","externalUrl":null,"permalink":"/byteglow/posts/llamaindex/","section":"Posts","summary":"","title":"LlamaIndex","type":"posts"},{"content":" 高级 RAG # 在预检索阶段，核心工作聚焦于索引优化与查询优化两大方向。\n索引优化旨在提升索引内容的品质，常见手段包括细化数据粒度、改良索引架构、补充元数据、优化对齐方式以及采用混合检索策略； 查询优化则着重将用户初始提问转化为更精准适配检索需求的表述，常借助查询重写、结构转换、语义扩展等技术达成目标。 后检索阶段的关键在于高效整合检索到的上下文与用户查询。由于直接将大量相关文档输入大语言模型易引发信息冗余，致使关键信息被淹没，因此，该阶段通常会运用块重排序、上下文压缩等方法，对检索结果进行优化处理 ，以提升信息处理效率与最终输出质量。\n1. Data Indexing Optimizations（数据索引优化） # 1.1 用于文本分块的滑动窗口 # 索引文本的一种简单方法是将文本拆分为 n 个部分，将它们转换为嵌入向量，然后将它们存储在向量数据库中。滑动窗口方法创建重叠的文本块，以确保在块的边界处不会丢失任何上下文信息。\nimport nltk from nltk.tokenize import sent_tokenize nltk.download(\u0026#39;punkt\u0026#39;) # Ensure the punkt tokenizer is downloaded nltk.download(\u0026#39;punkt_tab\u0026#39;) def sliding_window(text, window_size=3): \u0026#34;\u0026#34;\u0026#34; Generate text chunks using a sliding window approach. Args: text (str): The input text to chunk. window_size (int): The number of sentences per chunk. Returns: list of str: A list of text chunks. \u0026#34;\u0026#34;\u0026#34; sentences = sent_tokenize(text) print(sentences) return [\u0026#39; \u0026#39;.join(sentences[i:i+window_size]) for i in range(len(sentences) - window_size + 1)] # Example usage text = \u0026#34;This is the first sentence. Here comes the second sentence. And here is the third one. Finally, the fourth sentence.\u0026#34; chunks = sliding_window(text, window_size=3) for chunk in chunks: print(chunk) print(\u0026#34;-----\u0026#34;) # here, you can convert the chunk to embedding vector # and, save it to a vector database 1.2 元数据利用 # 元数据可以包含文档创建日期、作者或相关标签等信息，这些信息可用于在检索过程中筛选或确定文档的优先顺序，从而增强搜索过程。\nimport numpy as np import faiss documents = [ \u0026#34;Document 1 content here\u0026#34;, \u0026#34;Content of the second document\u0026#34;, \u0026#34;The third one has different content\u0026#34;, ] metadata = [ {\u0026#34;date\u0026#34;: \u0026#34;20230101\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;news\u0026#34;}, {\u0026#34;date\u0026#34;: \u0026#34;20230102\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;update\u0026#34;}, {\u0026#34;date\u0026#34;: \u0026#34;20230103\u0026#34;, \u0026#34;tag\u0026#34;: \u0026#34;report\u0026#34;}, ] # Dummy function to generate embeddings def generate_embeddings(texts): \u0026#34;\u0026#34;\u0026#34;Generate dummy embeddings for the sake of example.\u0026#34;\u0026#34;\u0026#34; return np.random.rand(len(texts), 128).astype(\u0026#39;float32\u0026#39;) # 128-dimensional embeddings # Generate embeddings for documents doc_embeddings = generate_embeddings(documents) # Create a FAISS index for the embeddings (using FlatL2 for simplicity) index = faiss.IndexFlatL2(128) # 128 is the dimensionality of the vectors index.add(doc_embeddings) # Add embeddings to the index # Example search function that uses metadata def search(query_embedding, metadata_key, metadata_value): \u0026#34;\u0026#34;\u0026#34;定义一个搜索函数，它不仅根据向量相似度查找文档，还会根据这些文档关联的元数据（比如日期、标签等）进行二次筛选。\u0026#34;\u0026#34;\u0026#34; k = 2 # Number of nearest neighbors to find distances, indices = index.search(np.array([query_embedding]), k) # Perform the search results = [] for idx in indices[0]: if metadata[idx][metadata_key] == metadata_value: results.append((documents[idx], metadata[idx])) return results # Generate a query embedding (in a real scenario, this would come from a similar process) query_embedding = generate_embeddings([\u0026#34;Query content here\u0026#34;])[0] # Search for documents tagged with \u0026#39;update\u0026#39; matching_documents = search(query_embedding, \u0026#39;tag\u0026#39;, \u0026#39;update\u0026#39;) print(matching_documents) 1.3 MultiVectorRetriever # 多向量检索器（MultiVectorRetriever ）允许每个文档存储多个向量，这在多种情况下非常有用。LangChain提供了一个基础的MultiVectorRetriever，使得查询这类设置变得简单。这种设置的主要复杂性在于如何为每个文档创建多个向量。这篇笔记涵盖了一些常见的创建向量的方法，并展示了如何使用MultiVectorRetriever。\n创建多个向量的方法：\n较小的分块：将文档分割成较小的部分，并对这些部分进行嵌入（例如ParentDocumentRetriever）。 摘要：为每个文档创建摘要，并将摘要（或代替整个文档）嵌入。 假设性问题：创建每个文档可能回答的假设性问题，并将这些问题（或代替文档）进行嵌入。 实现示例：\n向量存储：使用Chroma进行向量的存储。 文档存储：使用InMemoryByteStore 进行文档的存储。 检索器初始化：使用MultiVectorRetriever进行初始化，设置好文档和向量存储。 from langchain.retrievers.multi_vector import MultiVectorRetriever from langchain.storage import InMemoryByteStore from langchain_openai import OpenAIEmbeddings vectorstore = Chroma(collection_name=\u0026#34;full_documents\u0026#34;, embedding_function=OpenAIEmbeddings()) store = InMemoryByteStore() retriever = MultiVectorRetriever(vectorstore=vectorstore, byte_store=store, id_key=\u0026#34;doc_id\u0026#34;) 1.4 ParentDocumentRetriever # 在进行文档分割以便检索时，我们通常会遇到几种需求的冲突：\n精确的嵌入表示：我们希望拥有较小的文档，这样它们的嵌入能够更准确地反映其含义。如果文档过长，那么嵌入可能会失去其意义。 保留上下文：我们需要保证文档足够长，以保留每个文档块的上下文。 父文档检索器（ParentDocumentRetriever） 通过分割和存储小数据块来实现上述平衡。在检索时，它首先获取这些小数据块，然后查找这些块的父ID，并返回这些较大的文档。\n父文档指的是小数据块源自的文档。这可以是整个原始文档或者一个更大的数据块。\n主要流程如下\n索引阶段： 分割层级： 子文档（Child Documents）：对原始文档进行细粒度分块（例如：小段句子或小段文本），用于向量嵌入和精准检索。 父文档（Parent Documents）：对原始文档进行粗粒度分块（例如：整节、整页），用于提供完整上下文。 存储关系： 向量库（VectorStore）：存储子文档的嵌入向量。其元数据中记录对应的父文档ID。 文档存储（DocStore）：存储父文档的原始文本，并通过 ID 与子文档关联。 （可选）字节存储（ByteStore）：缓存子文档原文，加速返回结果（避免反复切分）。 检索阶段： Step 1 - 召回子文档：用查询向量在 VectorStore 中检索最相似的 K 个子文档。 Step 2 - 关联父文档：通过子文档的元数据 parent_doc_id 找到对应的 父文档原文。 最终返回：父文档内容（而非原子文档），提供完整上下文。 代码示例：\n数据准备 from langchain.embeddings import HuggingFaceBgeEmbeddings from langchain.document_loaders import TextLoader #创建BAAI的embedding bge_embeddings = HuggingFaceBgeEmbeddings(model_name=\u0026#34;BAAI/bge-small-zh-v1.5\u0026#34;) #创建loaders loaders = [ TextLoader(\u0026#34;./docs/华为智驾遥遥领先.txt\u0026#34;,encoding=\u0026#39;utf8\u0026#39;), TextLoader(\u0026#34;./docs/小米SU7遥遥领先.txt\u0026#34;,encoding=\u0026#39;utf8\u0026#39;), ] docs = [] for loader in loaders: docs.extend(loader.load()) 创建父文档检索器 from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.retrievers import ParentDocumentRetriever from langchain.storage import InMemoryStore from langchain.vectorstores import Chroma # 创建文档分割器,设置块大小为200 child_splitter = RecursiveCharacterTextSplitter(chunk_size=200) # 创建向量数据库对象 vectorstore = Chroma( collection_name=\u0026#34;full_documents\u0026#34;, embedding_function=bge_embeddings ) # 创建内存存储对象 store = InMemoryStore() #创建父文档检索器 retriever = ParentDocumentRetriever( vectorstore=vectorstore, #指定所使用的向量数据库 docstore=store, #原始文档存储器 child_splitter=child_splitter,#子文档分割器 ) #添加文档集 retriever.add_documents(docs, ids=None) 一旦完成添加原始文档的工作以后，所有的原始文档就会被child_splitter切割成一个个小的文档块，并且为小文档块与原始文档建立了索引关系，即通过小文档块的Id便能找到其对于的原始文档。\n检索 #搜索与用户问题相似度较高的子文档块 sub_docs = vectorstore.similarity_search(\u0026#34;小米SU7智能驾驶系统？\u0026#34;) print(sub_docs[0].page_content) #检索原始文档的全部内容 retrieved_docs = retriever.get_relevant_documents(\u0026#34;小米SU7智能驾驶系统？\u0026#34;) print(retrieved_docs[0].page_content) 1.5 RAPTOR # RAPTOR旨在解决超长文档的语义理解和精准检索难题。它通过递归构建树状文档结构，在复杂内容中实现多层级语义聚合，显著优于传统分块检索方法。\n基本流程：\n分块与嵌入。\n按 100 的大小对文本进行分块，如果一个句子长度超过 100，则直接将句子作为一个文本块，保证块内语义的一致性。（如何断句也很重要！） 对文本块进行 embedding。 递归的构建 RAPTOR 树。文本块及其 embedding 作为树的叶子节点。\n通过聚类把相似的块聚在一起。 利用语言模型为簇内的文本生成总结，并为总结生成 embedding，也作为树的一个节点。 递归执行上述过程。 查询。即如何检索相关的块。文中提供了两种策略：\nTree Traversal Retrieval。遍历树的每一层，剪枝并选择最相关的节点。 Collapsed Tree Retrieval。评估整个树中每个节点的相关性，找到最相关的几个。 RAPTOR 树构建流程： RAPTOR 树的两种查询策略：\n树遍历：树遍历方法首先根据其与查询嵌入的余弦相似度选择前 k 个最相关的根节点。这些选定节点的子节点在下一层被考虑，并且根据其与查询向量的余弦相似性再次从此池中选择前 k 个节点。重复此过程，直到我们到达叶节点。最后，将所有选定节点的文本连接起来，形成检索到的上下文。 折叠树方法：提供了一种更简单的方法来搜索相关信息，方法是同时考虑树中的所有节点，如图所示。这种方法不是逐层进行，而是将多层树展平为一层，实质上是将所有节点带到同一级别进行比较。 代码示例：代码 1.6 ColBERT # (a) 基于表示的相似性 (Representation-based Similarity): 在这种方法中,文档片段通过某些深度神经网络进行离线编码 (即预先处理),而查询片段则以类似的方式进行在线编码 (即实时处理)。然后计算查询与所有文档之间的成对相似性（通常是cos相似度）得分,并返回得分最高的几个文档。\n(b) 查询-文档交互 (Query-Document Interaction): 在这种方法中,通过使用 n-gram (即连续的 n 个词组成的序列) 计算查询和文档中所有单词之间的词语和短语级关系,作为一种特征工程的形式。这些关系被转换为交互矩阵,然后作为输入提供给卷积网络。\n(c) 全对全交互 (All-to-all Interaction): 这可以被视为方法 (b) 的泛化,它考虑了查询和文档内部以及彼此之间的所有成对交互。这是通过自注意力机制 (self-attention) 实现的。在实践中,通常使用 BERT (Bidirectional Encoder Representations from Transformers) 来实现,因为它是双向的,因此能够真正模拟所有成对交互,而不仅仅是因果关系。\nColBERT 建立在 BERT 模型之上，但它的设计与众不同。传统方法将句子编码为单一向量，而 ColBERT 为每个 token 生成独立的上下文嵌入向量。ColBERT 的核心思想是延迟交互，即在查询和文档编码过程中保持独立，但在计算相似度时进行细粒度的交互。\n考虑查询“What is BGE?”，单一向量可能因句子整体平均而削弱“BGE”的特征。而 ColBERT 赋予“BGE”独立向量，保留了细粒度信息，非常适合需要局部匹配的任务，如信息检索。\n延迟交互机制机制流程：\n分别生成查询 Q 和文档 D 的 token 向量集合，保持独立性。 对每个查询向量 qi，在文档向量中寻找最匹配的 dj，通过点积 qi⋅dj 计算相似性。 将所有查询 token 的最大匹配得分求和，得到最终得分。 为何延迟： 提前融合可能导致语义损失，尤其在长句子中。延迟交互则保留了每个 token 的独立性，直到需要匹配时才进行计算。\n简单示例： 查询“What is BGE?”与文档“BGE is a model”对比。ColBERT 能直接匹配“BGE”与“BGE”，避免整体平均带来的模糊。换句话说，可以理解成他是匹配局部与局部之间的相关性。\nColBERT 的查询编码过程：\n假设有一个查询 Q，其标记（token）为 q1， q2， \u0026hellip;， ql，处理步骤如下:\n将 Q 转换为 BERT 使用的 WordPiece 标记（token） (一种子词分词方法)。 在序列开头添加一个特殊的 [Q] 标记（token），紧随 BERT 的 [CLS] 标记（token）之后，用于标识查询的开始。 如果查询长度不足预设的 Nq 个标记（token），用 [mask] 标记（token）填充；若超过则截断。 将处理后的序列输入 BERT，然后通过卷积神经网络 (CNN) 处理，最后进行归一化。 最终输出的查询嵌入向量集合 Eq 可表示为:\n$Eq := Normalize(BERT([Q]， q0， q1， …， ql， [mask]， [mask]， …， [mask]))$\nColBERT 的文档编码过程：\n对于包含标记 d1， d2， \u0026hellip;， dn 的文档 D，处理步骤类似:\n在序列开头添加 [D] 标记，标识文档开始。 无需填充，直接输入 BERT 进行处理。 文档嵌入向量集合 Ed 可表示为:$ Ed := Filter(Normalize(BERT([D]， d0， d1， \u0026hellip;， dn)))$\nFilter用于去除与标点符号对应的嵌入，从而提升分析速度。这里的查询填充策略 (论文中称为\u0026quot;查询增强\u0026quot;)确保了所有查询长度一致，有利于批量处理。而 [Q] 和 [D] 标记则帮助模型区分输入类型，提高了处理效率。\n使用 ColBERT 查找最相关的前 K 个文档计算过程包括:\n批量点积计算:用于计算词语级别的相似度。每一个词都和整个文档进行计算\n最大池化 (max-pooling):在文档词语上进行操作,找出每个查询词语的最高相似度。\n求和:对查询词语的相似度分数进行累加,得出文档的总体相关性分数，公式如下：\n$$ S_{q,d} := \\sum_{i \\in [\\vert E_q \\vert]} \\max_{j \\in [\\vert E_d \\vert]} E_{q_i} \\cdot E_{d_j}^T \\tag{3} $$ 排序:根据总分对文档进行排序。\n代码示例：\nfrom ragatouille import RAGPretrainedModel RAG = RAGPretrainedModel.from_pretrained(\u0026#34;jinaai/jina-colbert-v2\u0026#34;) docs = [ \u0026#34;ColBERT is a novel ranking model that adapts deep LMs for efficient retrieval.\u0026#34;, \u0026#34;Jina-ColBERT is a ColBERT-style model but based on JinaBERT so it can support both 8k context length, fast and accurate retrieval.\u0026#34;, ] RAG.index(docs, index_name=\u0026#34;demo\u0026#34;) query = \u0026#34;What does ColBERT do?\u0026#34; results = RAG.search(query) 2. Query Enhancement（查询增强） # 2.1 查询重写 # 目的:使查询更加具体和详细，提高检索相关信息的可能性。\n方案:重写的确认样不仅与原始查询相似，而且还提供不同的角度或透视图，从而提高最终生成的质量和深度。\n示例代码：\nimport os from langchain_community.utilities import DuckDuckGoSearchAPIWrapper from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain_core.runnables import RunnablePassthrough from langchain_openai import ChatOpenAI os.environ[\u0026#34;DASHSCOPE_API_KEY\u0026#34;]=\u0026#34;\u0026lt;your-api-key\u0026gt;\u0026#34; os.environ[\u0026#34;ALIYUN_BASE_URL\u0026#34;]=\u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#34; model = ChatOpenAI( api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;), base_url=\u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#34;, model=\u0026#34;qwen-plus\u0026#34;, # 此处以qwen-plus为例，您可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models # other params... ) search = DuckDuckGoSearchAPIWrapper() base_template = \u0026#34;\u0026#34;\u0026#34;Answer the users question based only on the following context: \u0026lt;context\u0026gt; {context} \u0026lt;/context\u0026gt; Question: {question} \u0026#34;\u0026#34;\u0026#34; base_prompt = ChatPromptTemplate.from_template(base_template) def june_print(msg, res): print(\u0026#39;-\u0026#39; * 100) print(msg) print(res) def retriever(query): return search.run(query) def withoutRewrite(query): chain = ( {\u0026#34;context\u0026#34;: retriever, \u0026#34;question\u0026#34;: RunnablePassthrough()} | base_prompt | model | StrOutputParser() ) june_print( \u0026#39;The result of query:\u0026#39;, chain.invoke(query) ) june_print( \u0026#39;The result of the searched contexts:\u0026#39;, retriever(query) ) def withRewrite(query): rewrite_template = \u0026#34;\u0026#34;\u0026#34;Provide a better search query for \\ web search engine to answer the given question, end \\ the queries with ’**’. Question: \\ {x} Answer:\u0026#34;\u0026#34;\u0026#34; rewrite_prompt = ChatPromptTemplate.from_template(rewrite_template) def _parse(text): return text.strip(\u0026#34;**\u0026#34;) rewriter = rewrite_prompt |model | StrOutputParser() | _parse june_print( \u0026#39;Rewritten query:\u0026#39;, rewriter.invoke({\u0026#34;x\u0026#34;: query}) ) rewrite_retrieve_read_chain = ( { \u0026#34;context\u0026#34;: {\u0026#34;x\u0026#34;: RunnablePassthrough()} | rewriter | retriever, \u0026#34;question\u0026#34;: RunnablePassthrough(), } | base_prompt | model | StrOutputParser() ) june_print( \u0026#39;The result of the rewrite_retrieve_read_chain:\u0026#39;, rewrite_retrieve_read_chain.invoke(query) ) query = \u0026#34;The NBA champion of 2020 is the Los Angeles Lakers! Tell me what is langchain framework?\u0026#34; withRewrite(query) 2.1.1 假设文档嵌入（HyDE） # 论文《Precise Zero-Shot Dense Retrieval without Relevance Labels》提出了一种基于假设文档嵌入（HyDE）的方法，主要过程如图2所示：\n该过程主要分为四个步骤：\n使用LLM基于查询生成k个假设文档。这些生成的文件可能不是事实，也可能包含错误，但它们应该于相关文件相似（例如提及查询中的关键概念、逻辑结构）。此步骤的目的是通过LLM解释用户的查询。\n将生成的假设文档输入编码器，将其映射到密集向量f(dk)，编码器具有过滤功能，过滤掉假设文档中的噪声。这里，dk表示第k个生成的文档，f表示编码器操作。\n为提升鲁棒性，可从语言模型采样多个假设文档，将它们的编码向量取平均，或混合查询本身的向量（如公式），形成最终的查询向量。 $$ \\hat{v}_{q_{ij}} = \\frac{1}{N+1}[\\sum_{k=1}^N f(\\hat{d}_k) + f(q_{ij})] $$ 检索执行，用生成的查询向量在文档库向量空间中搜索最相似的向量，对应的真实文档即为检索结果。\n示例代码：\n# 导入依赖项 from llama_index.core import Settings from llama_index.llms.dashscope import DashScope from llama_index.embeddings.dashscope import DashScopeEmbedding # 设置大语言模型 Settings.llm = DashScope( api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;), model=\u0026#34;qwen-plus\u0026#34; ) # 配置嵌入模型 Settings.embed_model = DashScopeEmbedding( model_name=\u0026#34;text-embedding-v3\u0026#34;, api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;), embed_batch_size=10, ) # 配置日志 import logging import sys logging.basicConfig(stream=sys.stdout, level=logging.INFO) logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout)) # 导入核心功能 from llama_index.core import VectorStoreIndex, SimpleDirectoryReader from llama_index.core.indices.query.query_transform import HyDEQueryTransform from llama_index.core.query_engine import TransformQueryEngine from IPython.display import Markdown, display # 加载文档 documents = SimpleDirectoryReader(\u0026#34;../../data/pual_graham\u0026#34;).load_data() # 创建索引 index = VectorStoreIndex.from_documents(documents) # 示例查询 query_str = \u0026#34;what did paul graham do after going to RISD\u0026#34; # 原始查询（无转换） query_engine = index.as_query_engine() response = query_engine.query(query_str) display(Markdown(f\u0026#34;{response}\u0026#34;)) # 使用HyDE查询转换 hyde = HyDEQueryTransform(include_original=True) hyde_query_engine = TransformQueryEngine(query_engine, hyde) response = hyde_query_engine.query(query_str) display(Markdown(f\u0026#34;{response}\u0026#34;)) # 展示生成的假设文档 query_bundle = hyde(query_str) hyde_doc = query_bundle.embedding_strs[0] print(hyde_doc) # 潜在问题演示：当查询在无上下文情况下可能被误解时，HyDE 可能会产生误导 #若原始查询存在歧义或缺乏背景信息，LLM 生成的 “假设文档” 可能偏离真实文档的语义，导致检索结果错误。 query_str = \u0026#34;What is Bel?\u0026#34; # 原始查询 response = query_engine.query(query_str) display(Markdown(f\u0026#34;{response}\u0026#34;)) # HyDE转换查询 hyde = HyDEQueryTransform(include_original=True) hyde_query_engine = TransformQueryEngine(query_engine, hyde) response = hyde_query_engine.query(query_str) display(Markdown(f\u0026#34;{response}\u0026#34;)) # 开放式查询比较 #开放式查询缺乏具体约束，LLM 需自行推断作者观点。 query_str = \u0026#34;What would the author say about art vs. engineering?\u0026#34; # 原始查询 response = query_engine.query(query_str) display(Markdown(f\u0026#34;{response}\u0026#34;)) # HyDE转换查询 response = hyde_query_engine.query(query_str) display(Markdown(f\u0026#34;{response}\u0026#34;)) 2.1.2 Step-Back提示 # STEP-BACK PROMPING是一种简单的提示技术，使LLM能够从包含特定细节的实例中抽象、提取高级概念和基本原理。其思想是将“step-back问题”定义为从原始问题派生出的更抽象的问题。\n包括两个基本步骤：\n抽象：最初，我们提示LLM提出一个关于高级概念或原理的广泛问题，而不是直接响应查询。然后，我们检索关于所述概念或原理的相关事实。 推理：LLM可以根据这些关于高级概念或原理的事实推导出原始问题的答案。我们称之为抽象推理。 例如，如果查询包含大量细节，LLM很难检索相关事实来解决任务。如图5中的第一个例子所示，对于物理问题“如果温度增加2倍，体积增加8倍，理想气体的压力P会发生什么？”在直接推理该问题时，LLM可能会偏离理想气体定律的第一原理。\n同样，由于特定的时间范围限制，“Estella Leopold在1954年8月至1954年11月期间上过哪所学校？”这个问题很难直接解决。\n示例代码：\nimport os from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate from langchain_core.runnables import RunnableLambda from langchain_openai import ChatOpenAI from langchain_community.utilities import DuckDuckGoSearchAPIWrapper os.environ[\u0026#34;DASHSCOPE_API_KEY\u0026#34;]=\u0026#34;\u0026lt;your-api-key\u0026gt;\u0026#34; os.environ[\u0026#34;ALIYUN_BASE_URL\u0026#34;]=\u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#34; model = ChatOpenAI( api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;), base_url=\u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#34;, model=\u0026#34;qwen-plus\u0026#34;, # 此处以qwen-plus为例，您可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models # other params... ) def june_print(msg, res): print(\u0026#39;-\u0026#39; * 100) print(msg) print(res) question = \u0026#34;如果温度增加2倍，体积增加8倍，理想气体的压力P会发生什么？\u0026#34; base_prompt_template = \u0026#34;\u0026#34;\u0026#34;You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant. {normal_context} Original Question: {question} Answer:\u0026#34;\u0026#34;\u0026#34; base_prompt = ChatPromptTemplate.from_template(base_prompt_template) search = DuckDuckGoSearchAPIWrapper(max_results=4) def retriever(query): return search.run(query) def base(): base_chain = ( { # Retrieve context using the normal question (only the first 3 results) \u0026#34;normal_context\u0026#34;: RunnableLambda(lambda x: x[\u0026#34;question\u0026#34;]) | retriever, # Pass on the question \u0026#34;question\u0026#34;: lambda x: x[\u0026#34;question\u0026#34;], } | base_prompt | model | StrOutputParser() ) june_print(\u0026#39;The searched contexts of the original question:\u0026#39;, retriever(question)) june_print(\u0026#39;The result of base_chain:\u0026#39;, base_chain.invoke({\u0026#34;question\u0026#34;: question}) ) def step_back(): # Few Shot Examples examples = [ { \u0026#34;input\u0026#34;: \u0026#34;Could the members of The Police perform lawful arrests?\u0026#34;, \u0026#34;output\u0026#34;: \u0026#34;what can the members of The Police do?\u0026#34;, }, { \u0026#34;input\u0026#34;: \u0026#34;Jan Sindel’s was born in what country?\u0026#34;, \u0026#34;output\u0026#34;: \u0026#34;what is Jan Sindel’s personal history?\u0026#34;, }, ] # We now transform these to example messages example_prompt = ChatPromptTemplate.from_messages( [ (\u0026#34;human\u0026#34;, \u0026#34;{input}\u0026#34;), (\u0026#34;ai\u0026#34;, \u0026#34;{output}\u0026#34;), ] ) few_shot_prompt = FewShotChatMessagePromptTemplate( example_prompt=example_prompt, examples=examples, ) step_back_prompt = ChatPromptTemplate.from_messages( [ ( \u0026#34;system\u0026#34;, \u0026#34;\u0026#34;\u0026#34;You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\u0026#34;\u0026#34;\u0026#34;, ), # Few shot examples few_shot_prompt, # New question (\u0026#34;user\u0026#34;, \u0026#34;{question}\u0026#34;), ] ) step_back_question_chain = step_back_prompt | model | StrOutputParser() june_print(\u0026#39;The step-back question:\u0026#39;, step_back_question_chain.invoke({\u0026#34;question\u0026#34;: question})) june_print(\u0026#39;The searched contexts of the step-back question:\u0026#39;, retriever(step_back_question_chain.invoke({\u0026#34;question\u0026#34;: question}))) response_prompt_template = \u0026#34;\u0026#34;\u0026#34;You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant. {normal_context} {step_back_context} Original Question: {question} Answer:\u0026#34;\u0026#34;\u0026#34; response_prompt = ChatPromptTemplate.from_template(response_prompt_template) step_back_chain = ( { # Retrieve context using the normal question \u0026#34;normal_context\u0026#34;: RunnableLambda(lambda x: x[\u0026#34;question\u0026#34;]) | retriever, # Retrieve context using the step-back question \u0026#34;step_back_context\u0026#34;: step_back_question_chain | retriever, # Pass on the question \u0026#34;question\u0026#34;: lambda x: x[\u0026#34;question\u0026#34;], } | response_prompt | model | StrOutputParser() ) june_print(\u0026#39;The result of step_back_chain:\u0026#39;, step_back_chain.invoke({\u0026#34;question\u0026#34;: question})) step_back() 2.1.3 Multi Query Retriever # Multi Query Retriever 的核心思想是通过语言模型（LLM）对原始查询生成多个相关问题，并行检索相关文档，最后合并结果。\nfrom llama_index.embeddings.dashscope import DashScopeEmbedding from langchain.document_loaders import WebBaseLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.retrievers.multi_query import MultiQueryRetriever from langchain.chat_models import ChatOpenAI from langchain.vectorstores import Chroma from openai import OpenAI from typing import List from langchain.embeddings.base import Embeddings class AliyunEmbeddings(Embeddings): def __init__(self, client, model=\u0026#34;text-embedding-v3\u0026#34;): self.client = client self.model = model def embed_documents(self, texts: List[str]) -\u0026gt; List[List[float]]: \u0026#34;\u0026#34;\u0026#34;对多个文档进行嵌入\u0026#34;\u0026#34;\u0026#34; return [self.embed_query(text) for text in texts] def embed_query(self, text: str) -\u0026gt; List[float]: \u0026#34;\u0026#34;\u0026#34;对单个查询进行嵌入\u0026#34;\u0026#34;\u0026#34; response = self.client.embeddings.create( input=text, model=self.model ) return response.data[0].embedding # 1. 加载文档 loader = WebBaseLoader(\u0026#34;https://zh.wikipedia.org/zh-cn/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0\u0026#34;) docs = loader.load() # 2. 分割文本 text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50) splits = text_splitter.split_documents(docs) # 3. 创建向量数据库 client = OpenAI( # 若没有配置环境变量，请用阿里云百炼API Key将下行替换为：api_key= api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;), base_url=os.getenv(\u0026#34;ALIYUN_BASE_URL\u0026#34;), ) embeddings = AliyunEmbeddings(client) vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings) # 4. 创建基础检索器 base_retriever = vectorstore.as_retriever(search_kwargs={\u0026#34;k\u0026#34;: 5}) # 5. 创建 MultiQueryRetriever llm = ChatOpenAI( model=\u0026#34;deepseek-r1\u0026#34;, # 百炼支持的模型名称，例如 qwen-turbo 或 qwen-plus api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;), base_url=os.getenv(\u0026#34;ALIYUN_BASE_URL\u0026#34;), temperature=0.7, max_tokens=512 ) retriever = MultiQueryRetriever.from_llm( llm=llm, retriever=base_retriever, include_original=True ) # 6. 执行检索 question = \u0026#34;如何降低机器学习模型的过拟合？\u0026#34; docs = retriever.get_relevant_documents(question) print(f\u0026#34;检索到 {len(docs)} 个文档:\u0026#34;) for doc in docs: print(doc.metadata[\u0026#34;source\u0026#34;], \u0026#34;:\u0026#34;, doc.page_content[:100] + \u0026#34;...\u0026#34;) 可使用自定义模版\nretriever = MultiQueryRetriever.from_llm( llm=llm, retriever=base_retriever, prompt=CUSTOM_PROMPT # 使用自定义模板 ) 2.2 混合检索 # 使用 Elasticsearch 作为传统搜索机制，并使用 faiss 作为向量数据库进行语义搜索。代码 使用bm25 和向量检索实现。 def get_ensemble_retriever(self, k=3): bm25 = self.get_bm25_retriever(k) vector = self.get_vector_retriever(k) return EnsembleRetriever( retrievers=[bm25, vector], weights=[0.5, 0.5] ) 3. 重新排序和过滤 # 3.1 过滤策略 # 与查询相关性最高的信息可能被埋在大量的无关文档中，如果将所有这些文档都传递到大模型，可能导致更昂贵的调用费用，生成的响应也更差。\n3.1.2 LlamaIndex # 使用示例：\npostprocessor = SimilarityPostprocessor(similarity_cutoff=0.7) query_engine = index.as_query_engine(node_postprocessors=[postprocessor]) llamaIndex Node Postprocessor 包含以下内容：\nSimilarityPostprocessor：为每个检索结果按相似度打分，然后通过设置一个分数阈值进行过滤。\n向量余弦相似度：节点嵌入向量与查询嵌入向量的余弦距离（最常用）。 点积相似度：嵌入向量的点积结果，反映向量空间中的投影长度。 其他度量：如欧氏距离（取反后作为相似度）等，具体取决于索引类型（如VectorStoreIndex默认使用余弦相似度）。 KeywordNodePostprocessor：使用 spacy 的 短语匹配器（PhraseMatcher）对检索结果进行检查，按包含或不包含特定的关键字进行过滤。\npostprocessor = KeywordNodePostprocessor(required_keywords=[\u0026#34;keyword\u0026#34;] Sentence Embedding Optimizer ：使用 nltk.tokenize 对检索出的每一条结果进行分句，然后通过计算每个分句和用户输入的相似性来过滤和输入不相干的句子，有两种过滤方式：threshold_cutoff 是根据相似度阈值来过滤（比如只保留相似度 0.75 以上的句子），percentile_cutoff 是根据百分位阈值来过滤（比如只保留相似度高的前 50% 的句子）。这种后处理方法可以极大地减少 token 的使用。\nFixedRecencyPostprocessor ：假设检索结果中有时间字段，我们可以按时间排序，然后取 topK 结果，这种策略对回答一些有关最近信息的问题非常有效。\n实现逻辑\n显式时间标注：通过file_metadata为每个文档节点附加时间\ndocuments = SimpleDirectoryReader( input_files=[ .... ], file_metadata=get_file_metadata, ).load_data() EmbeddingRecencyPostprocessor ：和 FixedRecencyPostprocessor 类似，但无需手动提供时间戳，而是通过文档的嵌入向量本身推断近期性。\n假设文档的嵌入向量中隐含了时间相关的语义特征（例如，较新的文档可能在嵌入空间中具有与 “近期” 相关的潜在表示），通过模型（如预训练的语言模型）对嵌入向量进行分析，间接判断节点的近期性。 TimeWeightedPostprocessor ：通过时间权重对检索结果中的节点（Node）进行重新排序（Rerank），确保最新数据优先被检索到，解决多版本数据或动态更新内容的时效性问题。代码 实现逻辑\n时间元数据标注：通过节点的 metadata 字段存储时间戳（如 __last_accessed__），表示节点的最新更新或访问时间。\n时间衰减函数：time_decay（时间衰减因子，取值范围[0, 1]）控制权重随时间的衰减速度。time_decay=0.5 表示每单位时间（如天、小时）权重减半。\n#节点时间越接近当前时间，权重越高。 time_similarity = (1 - self.time_decay) ** hours_passed LongContextReorder ：当前的大模型并没有充分利用上下文中的信息：当相关信息出现在上下文的开头或结尾时，性能往往最高，而当模型必须在长上下文的中间访问相关信息时，性能会显著下降。\n重新排序过程如下：\n根据相关性得分对输入节点进行排序。\n然后，以交替模式对已排序的节点重新排序：\n偶数索引的节点放置在新列表的开头。\n奇数索引的节点放置在新列表的末尾。\n这种方法确保得分最高（最相关）的节点位于列表的开头和结尾，得分较低的节点位于中间。\n3.1.3 LangChain # LangChain 中上下文压缩同样解决此问了题。思路很简单：不是立即返回检索到的文档，而是使用给定查询的上下文对其进行压缩，以便只返回相关信息。这里的“压缩”既指单个文档内容的压缩，也指整个文档的过滤。上下文压缩检索器将查询传递给基本检索器，获取初始文档并将其通过文档压缩器传递。文档压缩器接受文档列表，通过减少文档内容或完全删除文档来缩短列表。\n示例代码：\nfrom langchain.retrievers import ContextualCompressionRetriever from langchain.retrievers.document_compressors import LLMChainExtractor from langchain_openai import OpenAI llm = OpenAI(temperature=0) compressor = LLMChainExtractor.from_llm(llm) compression_retriever = ContextualCompressionRetriever( base_compressor=compressor, base_retriever=retriever ) compressed_docs = compression_retriever.get_relevant_documents( \u0026#34;What did the president say about Ketanji Jackson Brown\u0026#34; ) pretty_print_docs(compressed_docs) LangChain 中的compressor：\nLLMChainExtractor：这个过滤器依次将检索文档和用户查询给大模型，让大模型从文档中抽取出和用户问题相关的片段，从而实现过滤的功能。\nfrom langchain.retrievers import ContextualCompressionRetriever from langchain.retrievers.document_compressors import LLMChainExtractor from langchain_openai import ChatOpenAI from langchain.chains.summarize import load_summarize_chain # 初始化 LLM llm =ChatOpenAI( openai_api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;), base_url=os.getenv(\u0026#34;ALIYUN_BASE_URL\u0026#34;), model=\u0026#39;qwen-plus\u0026#39;, ) # 创建提取器 compressor = LLMChainExtractor.from_llm(llm) # 提取关键信息 compression_retriever = ContextualCompressionRetriever( base_compressor=compressor, base_retriever=retriever ) compressed_docs = compression_retriever.get_relevant_documents( \u0026#34;What did the president say about Ketanji Jackson Brown\u0026#34; ) pretty_print_docs(compressed_docs) LLMChainFilter：这个过滤器相比 LLMChainExtractor 稍微简单一点，它直接让大模型判断文档和用户问题是否相关，而不是抽取片段，这样做不仅消耗更少的 token，而且处理速度更快，而且可以防止大模型对文档原始内容进行篡改。\nfrom langchain.retrievers.document_compressors import LLMChainFilter _filter = LLMChainFilter.from_llm(llm) compression_retriever = ContextualCompressionRetriever( base_compressor=_filter, base_retriever=retriever ) compressed_docs = compression_retriever.get_relevant_documents( \u0026#34;What did the president say about Ketanji Jackson Brown\u0026#34; ) pretty_print_docs(compressed_docs) EmbeddingsFilter：和 LlamaIndex 的 SimilarityPostprocessor 类似，计算每个文档和用户问题的相似度分数，然后通过设置一个分数阈值进行过滤。\nEmbeddingsRedundantFilter: 这个过滤器虽然名字和 EmbeddingsFilter 类似，但是实现原理是不一样的，它不是计算文档和用户问题之间的相似度，而是计算文档之间的相似度，然后把相似的文档过滤掉。\n将多个压缩器依次组合在一起：\n使用DocumentCompressorPipeline，我们还可以轻松地将多个压缩器依次组合在一起。\n在下面的示例中，我们首先将文档拆分为较小的块，然后删除冗余文档，然后根据与查询相关性进行过滤。\nfrom langchain.retrievers.document_compressors import DocumentCompressorPipeline from langchain_community.document_transformers import EmbeddingsRedundantFilter from langchain_text_splitters import CharacterTextSplitter splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0, separator=\u0026#34;. \u0026#34;) redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings) relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76) pipeline_compressor = DocumentCompressorPipeline( transformers=[splitter, redundant_filter, relevant_filter] ) compression_retriever = ContextualCompressionRetriever( base_compressor=pipeline_compressor, base_retriever=retriever ) compressed_docs = compression_retriever.get_relevant_documents( \u0026#34;What did the president say about Ketanji Jackson Brown\u0026#34; ) pretty_print_docs(compressed_docs) 3.2 重排序 # 在上面的过滤策略中，我们经常会用到 Embedding 来计算文档的相似性，然后根据相似性来对文档进行排序，这里的排序被称为 粗排，我们还可以使用一些专门的排序引擎对文档进一步排序和过滤，这被称为 精排。LlamaIndex 支持下面这些重排序策略：\n3.2.1 LlamaIndex # SentenceTransformerRerank ：比如 sentence-transformer 包中的 交叉编码器（Cross Encoder） 可以用来重新排序节点。\nfrom llama_index.core.postprocessor import SentenceTransformerRerank # 初始化重排器（使用交叉编码器模型，保留前3个节点） rerank = SentenceTransformerRerank( model=\u0026#34;cross-encoder/ms-marco-MiniLM-L-2-v2\u0026#34;, top_n=3 ) # 将重排器添加到查询引擎的后处理器链中 query_engine = index.as_query_engine( similarity_top_k=10, node_postprocessors=[rerank] ) Colbert Reranker :另一种实现本地重排序的是 ColBERT 模型，它是一种快速准确的检索模型，可以在几十毫秒内对大文本集合进行基于 BERT 的搜索。\nLLM Rerank ：我们还可以使用大模型来做重排序，将文档丢给大模型，然后让大模型对文档的相关性进行评分，从而实现文档的重排序。下面是 LlamaIndex 内置的用于重排序的 Prompt：\nRankGPT ：RankGPT 是 Weiwei Sun 等人在论文 Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents 中提出的一种基于大模型的 zero-shot 重排方法，它采用了排列生成方法和滑动窗口策略来高效地对段落进行重排序，具体内容可以参考 RankGPT 的源码 。\nRankLLMRerank ：RankLLM 和 RankGPT 类似，也是利用大模型来实现重排，只不过它的重点放在与 FastChat 兼容的开源大模型上，比如 Vicuna 和 Zephyr 等，并且对这些开源模型专门为重排任务进行了微调，比如 RankVicuna 和 RankZephyr 等。\n3.2.2 Langchain # 示例代码:\n以下代码展示了如何实现基于 HuggingFaceCrossEncoder 的文档重新排序器：\ndef local_reranker(retriever, top_n=3): model = HuggingFaceCrossEncoder(model_name=\u0026#34;BAAI/bge-reranker-base\u0026#34;) compressor = CrossEncoderReranker(model=model, top_n=top_n) return ContextualCompressionRetriever( base_compressor=compressor, base_retriever=retriever ) Bi-Encoder会用BERT对输入文本编码，再根据cosine相似度分数筛选文本。Cross-Encoder会直接计算两个句子的相关性分数。\n有一组预先定义好的句子对，并想对其进行打分时，就可以使用cross-Encoder\n需要在向量空间中获得句子嵌入以进行高效比较的情况，使用BiEncoder\nContextualCompressionRetriever：将交叉编码器包装为 Compressor 对象，负责对检索结果重排并截断。\n接收基础检索器返回的文档列表。 用交叉编码器计算每个文档相对于查询的相关性得分。 按得分降序排序，保留前 top_n 个文档。 ContextualCompressionRetriever：创建压缩检索器\n4. 使用知识图谱改进 RAG 检索 # 知识图谱是使用图结构来表示实体及其在现实世界中的关系并对其进行建模的一种技术方法。它将信息组织为节点（实体）和边（关系），形成一个有机网络，可以有效地存储、查询和分析复杂的知识。知识图谱的核心在于它使用三元组 （entity-relationship-entity） 来描述实体之间的关联。\n通过将文档提取到实体和关系中，知识图谱可以显著压缩文档块，从而可以将所有相关文档提交到LLM。\n知识图谱RAG与Base RAG区别\n知识图谱 RAG 使用图形结构来表示和存储信息，从而捕获实体之间的复杂关系，而Base RAG 通常使用矢量化文本数据。 知识图谱 RAG 通过图遍历和子图搜索来检索信息，而Base RAG 依赖于向量相似性搜索。 知识图谱 RAG 可以更好地理解实体之间的关系和层次结构，从而提供更丰富的上下文，而Base RAG 在处理复杂关系方面受到限制 from llama_index.graph_stores.neo4j import Neo4jGraphStore from llama_index.core import Settings from llama_index.llms.dashscope import DashScope from llama_index.embeddings.dashscope import DashScopeEmbedding # \u0026lt;--- Changed this line import os os.environ[\u0026#34;DASHSCOPE_API_KEY\u0026#34;]=\u0026#34;\u0026lt;your-api-key\u0026gt;\u0026#34; os.environ[\u0026#34;ALIYUN_BASE_URL\u0026#34;]=\u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#34; username = \u0026#34;neo4j\u0026#34; password = \u0026#34;12345678\u0026#34; url = \u0026#34;bolt://localhost:7687\u0026#34; database = \u0026#34;neo4j\u0026#34; graph_store = Neo4jGraphStore( username=username, password=password, url=url, database=database, ) #设置 llm Settings.llm = DashScope( api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;), model=\u0026#34;qwen-plus\u0026#34; ) # 配置嵌入模型 ⭐ 新增设置 Settings.embed_model = DashScopeEmbedding( model_name=\u0026#34;text-embedding-v3\u0026#34;, # 百炼嵌入模型 api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;), ) from llama_index.core import StorageContext, SimpleDirectoryReader, KnowledgeGraphIndex # 使用 SimpleDirectoryReader 加载文档数据。 documents = SimpleDirectoryReader(\u0026#34;./data\u0026#34;).load_data() # 使用 StorageContext 创建存储上下文对象，并传入图形存储对象/ storage_context = StorageContext.from_defaults(graph_store=graph_store) # 使用 KnowledgeGraphIndex 从文档创建知识图谱索引对象。 index = KnowledgeGraphIndex.from_documents( documents, storage_context=storage_context, max_triplets_per_chunk=2,#指定每个文档块将被提取为最多两个三元组。 include_embeddings=True,#表示提取的三元组将被转换为嵌入向量并保存。 ) query_engine = index.as_query_engine( include_text=True, response_mode=\u0026#34;tree_summarize\u0026#34;, embedding_mode=\u0026#34;hybrid\u0026#34;, similarity_top_k=5, verbose=True, ) response = query_engine.query(\u0026#34;白龍馬身世?\u0026#34;) print(f\u0026#34;Response: {response}\u0026#34;) 5. 多模态RAG # 方案 1：\n使用多模态嵌入（如 CLIP）对图像和文本进行嵌入通过相似度搜索同时检索图像和文本将原始图像和文本块传递给多模态大语言模型进行答案合成\n方案 2：\n使用多模态大语言模型（如 GPT-4V、LLaVA 或 FUYU-8b）从图像生成文本摘要对文本摘要进行嵌入和检索将文本块传递给大语言模型进行答案合成\n方案 3：\n使用多模态大语言模型（如 GPT-4V、LLaVA 或 FUYU-8b）从图像生成文本摘要嵌入并检索图像摘要（附带原始图像引用）将原始图像和文本块传递给多模态大语言模型进行答案合成\n6. 综合实战 # 6.1 数据集 # 数据集：使用数据为chinese-simplified-xlsum-v2新闻数据集，摘取chinese_simplified_train.jsonl 前八条\n6.2 数据加载与分块 # 数据清洗分块:由于数据集为jsonl格式，直接按照每个json单元分块\n数据加载与分块 - 针对特定JSONL格式优化\nclass NewsJsonlDataLoader: def __init__(self, file_path): self.file_path = file_path def load_and_chunk(self): \u0026#34;\u0026#34;\u0026#34;加载JSONL文件并分块处理\u0026#34;\u0026#34;\u0026#34; # 直接处理JSONL文件 loader = JSONLoader( file_path=self.file_path, jq_schema=\u0026#34;.\u0026#34;, # 整个对象作为文档 content_key=\u0026#34;text\u0026#34;, # 使用text字段作为内容 metadata_func=self.extract_metadata,#指定如何提取每条记录的元数据 json_lines=True # 处理JSONL格式 ) # 每个新闻作为一个分块 text_splitter = RecursiveCharacterTextSplitter( chunk_size=1000, chunk_overlap=100, separators=[\u0026#34;\\n\\n\u0026#34;, \u0026#34;\\n\u0026#34;, \u0026#34;。\u0026#34;, \u0026#34;！\u0026#34;, \u0026#34;？\u0026#34;, \u0026#34;．\u0026#34;] ) return loader.load_and_split(text_splitter=text_splitter) def extract_metadata(self, record: dict, metadata: dict) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34;提取元数据\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;id\u0026#34;: record.get(\u0026#34;id\u0026#34;, \u0026#34;\u0026#34;), \u0026#34;url\u0026#34;: record.get(\u0026#34;url\u0026#34;, \u0026#34;\u0026#34;), \u0026#34;title\u0026#34;: record.get(\u0026#34;title\u0026#34;, \u0026#34;\u0026#34;), \u0026#34;summary\u0026#34;: record.get(\u0026#34;summary\u0026#34;, \u0026#34;\u0026#34;), } RecursiveCharacterTextSplitter 原理：该工具以递归为核心机制。先按指定字符集顺序尝试分割文本，评估分割结果是否小于指定块大小，不满足则换用下一个字符，直到符合条件。\n6.3 向量存储 # # # 构建向量数据库（首次运行） # vector_store = Chroma.from_documents( # documents=documents, # embedding=embeddings, # persist_directory=\u0026#34;./chroma_db\u0026#34; # ) # # 显式保存到磁盘（Chroma 会自动保存，但建议显式调用） # vector_store.persist() # print(f\u0026#34;向量索引已保存至目录: ./chroma_db\u0026#34;) # 二次使用 # 从持久化目录加载向量存储 vector_store = Chroma( persist_directory=\u0026#34;./chroma_db\u0026#34;, # 与保存时相同的目录 embedding_function=embeddings ) 6.4 检索器系统 # class RetrievalSystem: def __init__(self, documents, vector_store): self.documents = documents self.vector_store = vector_store #关键词检索 def get_bm25_retriever(self, k=3): return BM25Retriever.from_documents( documents=self.documents, k=k ) #向量检索 def get_vector_retriever(self, k=3): return self.vector_store.as_retriever( search_type=\u0026#34;mmr\u0026#34;, search_kwargs={\u0026#34;k\u0026#34;: k} ) #混合检索 def get_ensemble_retriever(self, k=3): bm25 = self.get_bm25_retriever(k) vector = self.get_vector_retriever(k) return EnsembleRetriever( retrievers=[bm25, vector], weights=[0.5, 0.5] ) 6.5 重排 # class Reranker: #基于本地模型的重排 @staticmethod def local_reranker(retriever, top_n=3): model = HuggingFaceCrossEncoder(model_name=\u0026#34;BAAI/bge-reranker-base\u0026#34;) compressor = CrossEncoderReranker(model=model, top_n=top_n) return ContextualCompressionRetriever( base_compressor=compressor, base_retriever=retriever ) #基于阿里云api的重排 @staticmethod def cloud_reranker(retriever, top_n=3): # 实际使用需要替换为DashScope实现 compressor = DashScopeRerank() compression_retriever = ContextualCompressionRetriever( base_compressor=compressor, base_retriever=retriever ) return compression_retriever # 降级处理，直接返回原始检索器 6.6 质量评估 # class AnswerGrader: def __init__(self, llm): self.llm = llm def hallucination_grader(self): class GradeHallucinations(BaseModel): binary_score: str = Field(description=\u0026#34;答案是否虚构。(\u0026#39;yes\u0026#39; or \u0026#39;no\u0026#39;)\u0026#34;) instruction = \u0026#34;\u0026#34;\u0026#34; 你是一个评分人员，负责确认LLM的回复是否为虚构的。 以下会给你一个文件与相对应的LLM回复，请输出 \u0026#39;yes\u0026#39; or \u0026#39;no\u0026#39;作为判断结果。 \u0026#39;Yes\u0026#39; 代表LLM的回答是虚构的，未基于文件内容 \u0026#39;No\u0026#39; 则代表LLM的回答并未虚构，而是基于文件内容得出。 \u0026#34;\u0026#34;\u0026#34; prompt = ChatPromptTemplate.from_messages([ (\u0026#34;system\u0026#34;, instruction), (\u0026#34;human\u0026#34;, \u0026#34;文件: \\n\\n {documents} \\n\\n LLM 回复: {generation}\u0026#34;) ]) return prompt | self.llm.with_structured_output(GradeHallucinations) def answer_grader(self): class GradeAnswer(BaseModel): binary_score: str = Field(description=\u0026#34;答案是否回应问题。(\u0026#39;yes\u0026#39; or \u0026#39;no\u0026#39;)\u0026#34;) instruction = \u0026#34;\u0026#34;\u0026#34; 你是一个评分人员，负责确认答案是否回应了问题。 输出 \u0026#39;yes\u0026#39; or \u0026#39;no\u0026#39;。 \u0026#39;Yes\u0026#39; 代表答案确实回应了问题， \u0026#39;No\u0026#39; 则代表答案并未回应问题。 \u0026#34;\u0026#34;\u0026#34; prompt = ChatPromptTemplate.from_messages([ (\u0026#34;system\u0026#34;, instruction), (\u0026#34;human\u0026#34;, \u0026#34;用户问题: \\n\\n {question} \\n\\n 答案: {generation}\u0026#34;) ]) return prompt | self.llm.with_structured_output(GradeAnswer) 幻觉检测\n生成文本+RAG 文档给llm 判断 with_structured_output:LangChain 的方法，用于指定 LLM 的输出格式。 |:在 LangChain 中，| 是用于将两个组件串联的语法，类似于管道操作符。它将 hallucination_prompt 的输出传递给 structured_llm_grader。 answer_grader：确认答案是否回应问题\n6.7 RAG问答系统 # class RAGSystem: def __init__(self, retriever, client): self.retriever = retriever self.client = client def format_docs(self, docs: List[Document]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;格式化检索到的文档用于提示词\u0026#34;\u0026#34;\u0026#34; return \u0026#34;\\n\\n\u0026#34;.join([ f\u0026#34;标题: {doc.metadata[\u0026#39;title\u0026#39;]}\\n\u0026#34; f\u0026#34;日期: {doc.metadata.get(\u0026#39;date\u0026#39;, \u0026#39;未知\u0026#39;)}\\n\u0026#34; f\u0026#34;摘要: {doc.metadata.get(\u0026#39;summary\u0026#39;, \u0026#39;\u0026#39;)}\\n\u0026#34; f\u0026#34;内容: {doc.page_content[:500]}...\\n\u0026#34; f\u0026#34;来源: {doc.metadata[\u0026#39;url\u0026#39;]}\u0026#34; for doc in docs ]) def generate_answer(self, question: str) -\u0026gt; str: print(f\u0026#34;\\n检索相关文档: {question}...\u0026#34;) context_docs = self.retriever.get_relevant_documents(question) # 准备参考文档摘要 reference_summary = \u0026#34;参考文档摘要：\\n\u0026#34; for i, doc in enumerate(context_docs): title = doc.metadata.get(\u0026#39;title\u0026#39;, \u0026#39;无标题\u0026#39;) url = doc.metadata.get(\u0026#39;url\u0026#39;, \u0026#39;未知链接\u0026#39;) summary = doc.metadata.get(\u0026#39;summary\u0026#39;, \u0026#39;\u0026#39;) reference_summary += f\u0026#34;{i + 1}. 来源: {url} | 标题: {title} | 摘要: {summary}\\n\u0026#34; print(f\u0026#34;格式化 {len(context_docs)} 个相关文档...\u0026#34;) context = self.format_docs(context_docs) template = \u0026#34;\u0026#34;\u0026#34; 你是一个新闻分析助手，需要根据以下相关新闻片段回答问题。 如果问题涉及多个新闻，请综合多篇新闻内容进行回答。 回答时需要注明新闻来源，如可能请包含发布日期。 相关新闻片段： {context} \u0026#34;\u0026#34;\u0026#34; prompt = PromptTemplate.from_template(template) system_prompt = prompt.format(context=context) print(\u0026#34;生成回答...\u0026#34;) completion = self.client.chat.completions.create( model=\u0026#34;qwen-plus\u0026#34;, messages=[ {\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: system_prompt}, # 中文系统角色 {\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: question}, # 使用生成的提示 ] ) answer = completion.choices[0].message.content combined_response = f\u0026#34;{answer}\\n\\n{reference_summary}\u0026#34; return combined_response 6.7 主程序 # if __name__ == \u0026#34;__main__\u0026#34;: # 配置 JSONL_FILE = \u0026#34;../data/chinese-simplified-xlsum-v2/chinese_simplified_XLSum_v2.0/test.jsonl\u0026#34; # 替换为实际文件路径 # 步骤1: 加载数据并分块 print(\u0026#34;正在加载新闻数据...\u0026#34;) loader = NewsJsonlDataLoader(JSONL_FILE) documents = loader.load_and_chunk() print(f\u0026#34;成功加载 {len(documents)} 条新闻\u0026#34;) # 步骤2: 初始化LLM print(\u0026#34;初始化语言模型...\u0026#34;) client = OpenAI( # 若没有配置环境变量，请用阿里云百炼API Key将下行替换为：api_key= api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;), base_url=os.getenv(\u0026#34;ALIYUN_BASE_URL\u0026#34;), ) llm = ChatOpenAI( openai_api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;), base_url=os.getenv(\u0026#34;ALIYUN_BASE_URL\u0026#34;), model=\u0026#39;qwen-plus\u0026#39;, ) # 步骤3: 创建向量存储 print(\u0026#34;正在构建向量索引...\u0026#34;) # 创建 Chroma 向量存储并指定持久化目录 embeddings = AliyunEmbeddings(client=client, model=\u0026#34;text-embedding-v3\u0026#34;) # # 构建向量数据库（首次运行） # vector_store = Chroma.from_documents( # documents=documents, # embedding=embeddings, # persist_directory=\u0026#34;./chroma_db\u0026#34; # ) # # 显式保存到磁盘（Chroma 会自动保存，但建议显式调用） # vector_store.persist() # print(f\u0026#34;向量索引已保存至目录: ./chroma_db\u0026#34;) # 二次使用 # 从持久化目录加载向量存储 vector_store = Chroma( persist_directory=\u0026#34;./chroma_db\u0026#34;, # 与保存时相同的目录 embedding_function=embeddings ) print(\u0026#34;已成功加载本地向量存储\u0026#34;) # 步骤4: 创建检索系统 print(\u0026#34;构建检索系统...\u0026#34;) retrieval_sys = RetrievalSystem(documents, vector_store) # 选择检索方式: ensemble_retriever | self_query_retriever retriever = retrieval_sys.get_vector_retriever(k=3) print(\u0026#34;自查询检索器准备就绪\u0026#34;) # 步骤5: 添加重排 print(\u0026#34;添加结果重排...\u0026#34;) reranked_retriever = Reranker.local_reranker(retriever, top_n=2) # 步骤6: 创建RAG系统 print(\u0026#34;初始化问答系统...\u0026#34;) rag_system = RAGSystem(reranked_retriever, client) # 示例问题 questions = [ \u0026#34;心理健康新闻\u0026#34;, \u0026#34;足球新闻\u0026#34; ] for question in questions: print(\u0026#34;\\n\u0026#34; + \u0026#34;=\u0026#34; * 50) print(f\u0026#34;问题: {question}\u0026#34;) # 生成答案 answer = rag_system.generate_answer(question) print(\u0026#34;\\n回答:\u0026#34;) print(answer) # 步骤7: 回答质量评估 print(\u0026#34;\\n\u0026#34; + \u0026#34;-\u0026#34; * 50) print(\u0026#34;\\n质量评估:\u0026#34;) grader = AnswerGrader(llm) # # 幻觉评估 print(\u0026#34;\\n\u0026#34; + \u0026#34;-\u0026#34; * 50) hallucination_result = grader.hallucination_grader().invoke({ \u0026#34;documents\u0026#34;: \u0026#34;相关新闻摘要...\u0026#34;, \u0026#34;generation\u0026#34;: answer }) print(f\u0026#34;幻觉评估: {hallucination_result.binary_score}\u0026#34;) # 回答相关性评估 print(\u0026#34;\\n\u0026#34; + \u0026#34;-\u0026#34; * 50) relevance_result = grader.answer_grader().invoke({ \u0026#34;question\u0026#34;: question, \u0026#34;generation\u0026#34;: answer }) print(f\u0026#34;答案相关性: {relevance_result.binary_score}\u0026#34;) print(\u0026#34;\\n问答系统完成\u0026#34;) 参考 # https://juejin.cn/post/7400688776484798490 https://www.aneasystone.com/archives/2024/06/advanced-rag-notes.html https://docs.llamaindex.ai/en/stable/ https://python.langchain.com/docs/ https://zhuanlan.zhihu.com/p/701763569 ","date":"20 June 2025","externalUrl":null,"permalink":"/byteglow/posts/%E9%AB%98%E7%BA%A7rag/","section":"Posts","summary":"","title":"高级RAG","type":"posts"},{"content":" LangChain 简易教程 # 1. LangChain 核心概念解析 # 1.1 框架定位与价值 # LangChain 是专为大语言模型（LLM）应用开发设计的全生命周期框架，通过模块化组件降低开发门槛，实现从原型设计到生产部署的全流程支持。其核心优势体现在：\n开发效率：提供预制的链条（Chains）、代理（Agents）等组件，无需从零构建复杂逻辑 可观测性：通过 LangSmith 实现全流程监控、评估与优化 工程化能力：借助 LangServe 将模型服务转化为标准 API 1.2 生态组件架构 # 核心库体系：\n组件名称 功能定位 langchain-core 定义基础抽象（如 LLM 接口、LCEL 表达式语言） langchain-community 集成第三方工具（文档加载、向量数据库等） langchain-openai 封装 OpenAI 兼容接口，支持阿里云百炼等第三方服务 langchain 包含链、代理、检索策略等核心应用逻辑 langgraph 支持多角色对话系统的图结构建模 生态工具链：\nLangSmith：LLM 应用调试、测试与监控平台，支持追踪提示词、评估生成质量 LangServe：轻量化服务部署工具，可将 Chain 转换为 REST API 1.3 关键组件 # 关键组件解释:\nPrompts：Prompts用来管理 LLM 输入的工具，在从 LLM 获得所需的输出之前需要对提示进行相当多的调整，最终的Promps可以是单个句子或多个句子的组合，它们可以包含变量和条件语句。 Chains：是一种将LLM和其他多个组件连接在一起的工具，以实现复杂的任务。 Agents：是一种使用LLM做出决策的工具，它们可以执行特定的任务并生成文本输出。Agents通常由三个部分组成：Action、Observation和Decision。Action是代理执行的操作，Observation是代理接收到的信息，Decision是代理基于Action和Observation做出的决策。 Memory：是一种用于存储数据的工具，由于LLM 没有任何长期记忆，它有助于在多次调用之间保持状态。 2. 环境配置与依赖管理 # 2.1 基础安装 # pip install langchain # 安装核心框架及基础依赖 注意：默认安装不包含第三方集成依赖，需根据场景额外安装：\n向量数据库：pip install chroma-client langchain-chroma OpenAI 兼容接口：pip install openai 文档处理：pip install python-docx 2.2 核心模块独立安装 # pip install langchain-core # 基础抽象与表达式语言 pip install langchain-community # 第三方集成组件 pip install langgraph # 多角色对话图建模 3. 快速入门 # 3.1 API 配置与模型调用 # 阿里云百炼接口示例：\nimport os from openai import OpenAI # 方式一：环境变量配置（推荐生产环境） os.environ[\u0026#39;DASHSCOPE_API_KEY\u0026#39;] = \u0026#39;\u0026lt;your-api-key\u0026gt;\u0026#39; os.environ[\u0026#39;ALIYUN_BASE_URL\u0026#39;] = \u0026#39;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#39; # 方式二：显式传参（开发测试用） client = OpenAI( api_key=\u0026#39;\u0026lt;your-api-key\u0026gt;\u0026#39;, base_url=os.getenv(\u0026#34;ALIYUN_BASE_URL\u0026#34;) ) # 调用模型生成回答 completion = client.chat.completions.create( model=\u0026#34;qwen-plus\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;你是营销领域专家\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;如何设计数字化营销话术？\u0026#34;} ] ) print(completion.choices[0].message.content) 在与LLM交互时，消息通常包含不同的角色，每个角色有其特定的含义和使用场景 ：\n角色 用途说明 最佳实践示例 system 设定助手行为基线 \u0026ldquo;你是电商客服，需用简洁话术解答售后问题\u0026rdquo; user 用户输入内容 \u0026ldquo;请问这款产品支持7天无理由退货吗？\u0026rdquo; assistant 历史回复记录 用于上下文记忆，避免重复提问 tool 工具调用结果 RAG 流程中检索到的文档片段 在调用LLM时，可以设置多种参数来控制生成文本的特性，常见参数说明：\n参数名 影响维度 推荐取值范围 典型应用场景 temperature 输出随机性 0.0（确定性）~1.0（创意性） 代码生成用0.1，营销文案用0.7 max_tokens 输出长度限制 ≤ 模型上下文长度 GPT-3.5 建议 ≤ 4000，Qwen 建议 ≤ 8000 presence_penalty 避免重复用词 0.0~1.0 长文本生成时设为0.5防止内容堆砌 streaming 流式输出 True/False 前端实时展示时启用，提升交互体验 使用 OpenAI 兼容接口获取可调用模型列表:\nimport os from openai import OpenAI # 初始化客户端 client = OpenAI( api_key=\u0026#39;\u0026lt;your-api-key\u0026gt;\u0026#39;, # 替换为你的 API Key base_url=\u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#34; ) # 获取模型列表（DashScope 支持这个 endpoint） models = client.models.list() # 打印可用模型名称 for model in models: print(model.id) 3.2 ChatOpenAI 类 # ChatOpenAI 是 LangChain 中最常用的类之一，用于调用 OpenAI 或兼容 OpenAI 协议的服务（如阿里云 DashScope、Moonshot 等）。\n示例初始化代码 ：\nfrom langchain_openai import ChatOpenAI llm = ChatOpenAI( model=\u0026#34;gpt-3.5-turbo\u0026#34;, temperature=0.7, max_tokens=256, verbose=True, openai_api_key=\u0026#39;\u0026lt;your-api-key\u0026gt;\u0026#39;, openai_api_base=\u0026#34;https://api.openai.com/v1 \u0026#34; ) 参数说明 ：\n参数名 类型 描述 model str 使用的模型名称，如 \u0026quot;gpt-3.5-turbo\u0026quot;、\u0026quot;deepseek-chat\u0026quot; 等。对于百炼，可设置为 \u0026quot;qwen-max\u0026quot;、\u0026quot;qwen-turbo\u0026quot; 等 。 temperature float 控制输出随机性，值越高越随机，范围 [0, 1]，推荐 0.7 左右 。 max_tokens int 控制模型生成的最大 token 数量 。 verbose bool 是否打印中间日志信息（调试用） 。 openai_api_key str API 密钥，用于认证 。 openai_api_base str 模型服务地址，如果是自定义服务（如阿里云），要设置为对应 URL 。 n int 一次生成多少个候选回复，默认是 1 。 streaming bool 是否启用流式输出（逐字返回结果） 。 request_timeout float or tuple 请求超时时间，防止卡死 。 3.3 记忆模块 # ChatMessageHistory是一个非常轻量的用于存取HumanMessages/AIMessages等消息的工具类。\nfrom langchain.memory import ChatMessageHistory history = ChatMessageHistory() history.add_user_message(\u0026#34;hi!\u0026#34;) history.add_ai_message(\u0026#34;whats up?\u0026#34;) # [HumanMessage(content=\u0026#39;hi!\u0026#39;, additional_kwargs={}), AIMessage(content=\u0026#39;whats up?\u0026#39;, additional_kwargs={})] print(history.messages) ConversationBufferMemory 是 LangChain 中最基础的记忆模块，它会将所有对话历史保存在一个缓冲区里 。\n示例初始化代码 ：\nfrom langchain.memory import ConversationBufferMemory memory = ConversationBufferMemory( memory_key=\u0026#34;chat_history\u0026#34;, # 存储对话历史的键名 input_key=\u0026#34;user_input\u0026#34;, # 用户输入的键名 output_key=\u0026#34;ai_response\u0026#34;, # AI 输出的键名 return_messages=False, # 返回字符串格式而不是 Message 对象列表 human_prefix=\u0026#34;User\u0026#34;, # 用户前缀 ai_prefix=\u0026#34;AI\u0026#34; # AI 前缀 ) 示例代码：\nfrom langchain.chains import LLMChain from langchain.memory import ConversationBufferMemory from langchain.prompts import PromptTemplate from langchain_openai import OpenAI template = \u0026#34;\u0026#34;\u0026#34;You are a chatbot having a conversation with a human. {chat_history} Human: {human_input} Chatbot:\u0026#34;\u0026#34;\u0026#34; prompt = PromptTemplate( input_variables=[\u0026#34;chat_history\u0026#34;, \u0026#34;human_input\u0026#34;], template=template ) memory = ConversationBufferMemory(memory_key=\u0026#34;chat_history\u0026#34;) llm = ChatOpenAI( model=\u0026#34;deepseek-r1\u0026#34;, # 百炼支持的模型名称，例如 qwen-turbo 或 qwen-plus api_key=\u0026#34;\u0026lt;your-api-key\u0026gt;\u0026#34;, base_url=\u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#34;, temperature=0.7, max_tokens=512 ) llm_chain = LLMChain( llm=llm, prompt=prompt, verbose=True, memory=memory, ) # Hello there! How are you? print(llm_chain.predict(human_input=\u0026#34;Hi, my friend\u0026#34;)) 在Agent中使用内存\nfrom langchain.agents import AgentExecutor, Tool, ZeroShotAgent from langchain.chains import LLMChain from langchain.memory import ConversationBufferMemory from langchain_community.utilities import SerpAPIWrapper from langchain_openai import ChatOpenAI # 定义Tool # 需要定义环境变量 export GOOGLE_API_KEY=\u0026#34;\u0026#34;, 在网站上注册并生成API Key: https://serpapi.com/searches search = SerpAPIWrapper() tools = [ Tool( name=\u0026#34;Search\u0026#34;, func=search.run, description=\u0026#34;useful for when you need to answer questions about current events\u0026#34;, ) ] # 定义Prompt prefix = \u0026#34;\u0026#34;\u0026#34;Have a conversation with a human, answering the following questions as best you can. You have access to the following tools:\u0026#34;\u0026#34;\u0026#34; suffix = \u0026#34;\u0026#34;\u0026#34;Begin!\u0026#34; {chat_history} Question: {input} {agent_scratchpad}\u0026#34;\u0026#34;\u0026#34; prompt = ZeroShotAgent.create_prompt( tools, prefix=prefix, suffix=suffix, input_variables=[\u0026#34;input\u0026#34;, \u0026#34;chat_history\u0026#34;, \u0026#34;agent_scratchpad\u0026#34;], ) # 定义Memory memory = ConversationBufferMemory(memory_key=\u0026#34;chat_history\u0026#34;) llm =ChatOpenAI( model=\u0026#34;qwen-plus\u0026#34;, # 百炼支持的模型名称，例如 qwen-turbo 或 qwen-plus api_key=\u0026#34;\u0026lt;your-api-key\u0026gt;\u0026#34;, base_url=\u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#34;, temperature=0.7, max_tokens=512 ) # 定义LLMChain llm_chain = LLMChain(llm=llm, prompt=prompt) # 定义Agent agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True) agent_chain = AgentExecutor.from_agent_and_tools( agent=agent, tools=tools, verbose=True, memory=memory ) agent_chain.run(input=\u0026#34;How many people live in canada?\u0026#34;) 3.4 提示模版 # PromptTemplate 用于格式化单个字符串，通常用于较简单的输入 。\n定义方式一 ：\nfrom langchain.prompts import PromptTemplate prompt_template = PromptTemplate.from_template(\u0026#34;模板字符串\u0026#34;) 定义方式二 ：\nprompt = PromptTemplate( input_variables=[\u0026#34;name\u0026#34;, \u0026#34;topic\u0026#34;], template=\u0026#34;你好，{name}，请谈谈你对 {topic} 的看法。\u0026#34;, ) print(prompt.format(name=\u0026#34;小明\u0026#34;, topic=\u0026#34;人工智能\u0026#34;)) 参数说明 ：\n参数名 描述 template 原始提示词模板，包含 {变量} 占位符 input_variables 所有模板中使用的变量名 partial_variables 可选，预设部分变量值 validate_template 是否验证模板变量一致性 3.5 缓存 # 如果多次请求的返回一样，就可以考虑使用缓存，一方面可以减少对API调用次数节省token消耗，一方面可以加快应用程序的速度。\nfrom langchain.cache import InMemoryCache import time import langchain from langchain.llms import OpenAI llm = OpenAI(model_name=\u0026#34;text-davinci-002\u0026#34;, n=2, best_of=2) langchain.llm_cache = InMemoryCache() s = time.perf_counter() llm(\u0026#34;Tell me a joke\u0026#34;) elapsed = time.perf_counter() - s # executed first in 2.18 seconds. print(\u0026#34;\\033[1m\u0026#34; + f\u0026#34;executed first in {elapsed:0.2f} seconds.\u0026#34; + \u0026#34;\\033[0m\u0026#34;) llm(\u0026#34;Tell me a joke\u0026#34;) # executed second in 0.72 seconds. elapsed2 = time.perf_counter() - elapsed print(\u0026#34;\\033[1m\u0026#34; + f\u0026#34;executed second in {elapsed2:0.2f} seconds.\u0026#34; + \u0026#34;\\033[0m\u0026#34;) 3.6 流式输出 # 如果需要流式输出，使用chain.stream()即可，需要注意的是使用前需要确认具体的某个Output Parser是否支持流式输出功能。\n# 创建Model from langchain_openai import ChatOpenAI model = ChatOpenAI() # 创建output_parser(输出) from langchain_core.output_parsers import JsonOutputParser from langchain_core.pydantic_v1 import BaseModel, Field class MathProblem(BaseModel): question: str = Field(description=\u0026#34;the question\u0026#34;) answer: str = Field(description=\u0026#34;the answer of question\u0026#34;) steps: str = Field(description=\u0026#34;the resolve steps of question\u0026#34;) output_parser = JsonOutputParser(pydantic_object=MathProblem) # 创建prompt(输入) from langchain.prompts import PromptTemplate prompt = PromptTemplate( template=\u0026#34;You are good at math, please answer the user query.\\n{format_instructions}\\n{query}\\n\u0026#34;, input_variables=[\u0026#34;query\u0026#34;], partial_variables={\u0026#34;format_instructions\u0026#34;: output_parser.get_format_instructions()}, ) # 创建Chain并链式调用 chain = prompt | model | output_parser print(chain.invoke({\u0026#34;query\u0026#34;: \u0026#34;1+1=?\u0026#34;})) # 使用流式输出 for s in chain.stream({\u0026#34;query\u0026#34;: \u0026#34;1+1=?\u0026#34;}): # \u0026lt;\u0026lt;------ print(s) 3.7 Agent # 很多时候有些功能是可以复用的。也就是说我们可以把基于LLM实现的一个功能抽象成一个可复用的模块，没错，它就是Agent ！\nAgent的核心思想是基于LLM大语言模型做一系列的操作，并把这一系列操作抽象成一个可复用的功能！明白了这个，就会对后面Agent的理解有很大帮助，让我们把结构精简为下图所示\nPlanning ：Agent的规划阶段涉及确定如何利用LLM大语言模型以及其他工具来完成特定任务。这包括确定所需的输入和输出，以及选择适当的工具和策略。 Memory ：在记忆阶段，Agent需要能够存储和访问过去的信息，以便在当前任务中使用。这包括对过去对话或交互的记忆，以及对相关实体和关系的记忆。 Tools ：工具是Agent执行任务所需的具体操作。这可能涉及到执行搜索、执行特定编程语言代码、执行数据处理等操作。这些工具可以是预定义的函数或API如search(), python_execute()等 Action ：在执行阶段，Agent利用选择的工具执行特定的动作，以完成规划阶段确定的任务。这可能包括生成文本、执行计算、操作数据等。动作的执行通常是基于规划阶段的决策和记忆阶段的信息。 Agent类型按照模型类型、是否支持聊天历史、是否支持函数并行调用等维度的不同，主要分为以下几种不同的Agent，更多可以参考agent_types文档 ：\nOpenAI functions ：基于OpenAI Function的Agent OpenAI tools ：基于OpenAI Tool的Agent XML Agent ：有些LLM模型很适合编写和理解XML（比如Anthropic’s Claude），所以可以使用XML Agent JSON Chat Agent ：有些LLM模型很适合编写和理解JSON，所以可以使用JSON Agent Structured chat Agent ：使用结构化的聊天Agent可以使用多输入的工具 ReAct Agent ：基于ReAct 逻辑的Agent Agent可以使用搜索工具来获取特定主题的信息，使用语言处理工具来理解和生成文本，使用编程执行工具来执行特定的代码等。这些工具允许Agent从外部获取所需的信息，并对外部环境产生影响。在这种情况下它的工作流程如下所示 ：\n用户发起请求，Agent接收请求 Agent会把 System Text + User Text + Tools/Functions 一起传递给LLM（如调用ChatGPT接口） 由于LLM发现传递了Tools/Functions参数，所以首次LLM只返回应该调用的函数（如search_func） Agent会自己调用对应的函数（如search_func）并获取到函数的返回结果（如search_result） Agent把函数的返回结果并入到上下文中，最后再把 System Text + User Text + search_result 一起传递给LLM LLM把结果返回给Agent Agent再把结果返回给用户 在Langchain中，Tools是一个在抽象层定义的类，它具备一些如name/description/args_schema/func等之类的基础属性，也支持使用@tool自定义Tool工具，更多请参看源码 和接口文档 ，同时框架内部也集成了很多开箱即用的Tools 和ToolKits工具集 。\n使用@tool注解自定义一个Tool工具\nfrom langchain.tools import tool @tool def search(query: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Look up things online.\u0026#34;\u0026#34;\u0026#34; return \u0026#34;LangChain\u0026#34; print(search) 3.8 Chains # 3.9 Callback # Langchain提供了一系列系统级别的回调函数，也就是在整个生命周期内的Hook钩子，以便于用户在应用层做日志、监控等其他处理。\nfrom langchain_core.callbacks import BaseCallbackHandler from langchain_openai import ChatOpenAI from langchain_core.prompts import ChatPromptTemplate from langchain_core.output_parsers import StrOutputParser # 1. 定义一个自定义回调处理器 class MyLoggingCallbackHandler(BaseCallbackHandler): def on_llm_start(self, serialized, prompts, **kwargs): \u0026#34;\u0026#34;\u0026#34;当 LLM 开始时调用。\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;--- LLM 开始 ---\u0026#34;) print(f\u0026#34;Serialized: {serialized}\u0026#34;) print(f\u0026#34;Prompts: {prompts}\u0026#34;) def on_llm_end(self, response, **kwargs): \u0026#34;\u0026#34;\u0026#34;当 LLM 结束时调用。\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;--- LLM 结束 ---\u0026#34;) print(f\u0026#34;Response: {response.generations[0][0].text}\u0026#34;) def on_chain_start(self, serialized, inputs, **kwargs): \u0026#34;\u0026#34;\u0026#34;当 Chain 开始时调用。\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;--- Chain \u0026#39;{serialized.get(\u0026#39;name\u0026#39;, \u0026#39;Unnamed Chain\u0026#39;)}\u0026#39; 开始 ---\u0026#34;) print(f\u0026#34;Inputs: {inputs}\u0026#34;) def on_chain_end(self, outputs, **kwargs): \u0026#34;\u0026#34;\u0026#34;当 Chain 结束时调用。\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;--- Chain 结束 ---\u0026#34;) print(f\u0026#34;Outputs: {outputs}\u0026#34;) # 2. 初始化 LLM 和 Prompt Template llm = ChatOpenAI( model=\u0026#34;qwen-plus\u0026#34;, # 百炼支持的模型名称，例如 qwen-turbo 或 qwen-plus api_key=\u0026#34;\u0026lt;your-api-key\u0026gt;\u0026#34;, base_url=\u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#34;, temperature=0.7, max_tokens=512 ) prompt = ChatPromptTemplate.from_messages([ (\u0026#34;system\u0026#34;, \u0026#34;你是一个乐于助人的AI助手。\u0026#34;), (\u0026#34;user\u0026#34;, \u0026#34;{question}\u0026#34;) ]) # 3. 创建一个链 chain = prompt | llm | StrOutputParser() # 4. 在 Chain 调用时传入回调 print(\u0026#34;\\n--- 示例 1: 通过 invoke 传入回调 ---\u0026#34;) response = chain.invoke( {\u0026#34;question\u0026#34;: \u0026#34;解释一下光合作用。\u0026#34;}, config={\u0026#34;callbacks\u0026#34;: [MyLoggingCallbackHandler()]} ) print(f\u0026#34;最终响应: {response}\u0026#34;) # 5. 也可以在构建 LLM 时传入回调 (只对该 LLM 有效) print(\u0026#34;\\n--- 示例 2: 在 LLM 构造函数中传入回调 ---\u0026#34;) llm_with_callback = ChatOpenAI( temperature=0.7, model=\u0026#34;qwen-plus\u0026#34;, # 百炼支持的模型名称，例如 qwen-turbo 或 qwen-plus api_key=\u0026#34;\u0026lt;your-api-key\u0026gt;\u0026#34;, base_url=\u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#34;, callbacks=[MyLoggingCallbackHandler()]) chain_with_llm_callback = prompt | llm_with_callback | StrOutputParser() response_2 = chain_with_llm_callback.invoke({\u0026#34;question\u0026#34;: \u0026#34;讲个笑话。\u0026#34;}) print(f\u0026#34;最终响应: {response_2}\u0026#34;) 3.10 LCEL # LCEL（LangChain Expression Language）是一种构建复杂链的简便方法，语法是使用|或运算符自动创建Chain后，即可完成链式操作。这在背后的原理是python的__ror__魔术函数，比如chain = prompt | model就相当于chain = prompt.__or__(model)。\n下面看一个简单的LCEL代码，按照传统的方式创建prompt/model/output_parser，然后再使用|或运算符创建了一个Chain，它自动把这3个组件链接在了一起，这都是在底层实现的，对应用层十分友好 ！\nfrom langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain_openai import ChatOpenAI prompt = ChatPromptTemplate.from_template(\u0026#34;tell me a short joke about {topic}\u0026#34;) model = ChatOpenAI() output_parser = StrOutputParser() chain = prompt | model | output_parser print(chain.invoke({\u0026#34;topic\u0026#34;: \u0026#34;math\u0026#34;})) 在LCEL的底层，主要是实现了一套通用的Runnable协议，只要各类组件遵循并实现此协议，便可以自动完成链式组合和调用。\n统一的接口 ：每个LCEL对象都实现该Runnable接口，该接口定义了一组通用的调用方法（invoke、batch、stream、ainvoke、 …）。这使得LCEL对象链也可以自动支持这些调用。也就是说，每个LCEL对象链本身就是一个LCEL对象。 组合原语 ：LCEL提供了许多原语（比如__ror__魔术函数），可以轻松组合链、并行化组件、添加后备、动态配置链内部等等。 示例代码：\nfrom langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain_openai import ChatOpenAI prompt = ChatPromptTemplate.from_template(\u0026#34;tell me a short joke about {topic}\u0026#34;) model = ChatOpenAI( model=\u0026#34;qwen-plus\u0026#34;, # 百炼支持的模型名称，例如 qwen-turbo 或 qwen-plus api_key=\u0026#34;\u0026lt;your-api-key\u0026gt;\u0026#34;, base_url=\u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#34;, temperature=0.7, max_tokens=512 ) output_parser = StrOutputParser() chain = prompt | model | output_parser # invoke: 普通输出 print(chain.invoke({\u0026#34;topic\u0026#34;: \u0026#34;math\u0026#34;})) # ainvoke: 异步输出 chain.ainvoke({\u0026#34;topic\u0026#34;: \u0026#34;math\u0026#34;}) # stream: 流式输出 for chunk in chain.stream({\u0026#34;topic\u0026#34;: \u0026#34;math\u0026#34;}): print(chunk, end=\u0026#34;\u0026#34;, flush=True) # Batch: 批量输入 print(chain.batch([{\u0026#34;topic\u0026#34;: \u0026#34;math\u0026#34;}, {\u0026#34;topic\u0026#34;: \u0026#34;English\u0026#34;}])) 4. AI 营销大模型实战 # 4.1 文档处理与向量化流程 # 4.1.1 步骤1：加载与分割文档 # #加载模型 import os from openai import OpenAI os.environ[\u0026#34;DASHSCOPE_API_KEY\u0026#34;]=\u0026#34;\u0026lt;your-api-key\u0026gt;\u0026#34; os.environ[\u0026#34;ALIYUN_BASE_URL\u0026#34;]=\u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#34; client = OpenAI( # 若没有配置环境变量，请用阿里云百炼API Key将下行替换为：api_key= api_key=os.getenv(\u0026#34;DASHSCOPE_API_KEY\u0026#34;), base_url=os.getenv(\u0026#34;ALIYUN_BASE_URL\u0026#34;), ) # 加载文档 from langchain.document_loaders import Docx2txtLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from typing import List # 使用 Docx2txtLoader 加载文档 loader = Docx2txtLoader(\u0026#34;database/企业数字化转型营销话术-tips.docx\u0026#34;) docs = loader.load() # 分割文本 text_splitter = RecursiveCharacterTextSplitter( chunk_size=500, chunk_overlap=50, length_function=len, is_separator_regex=False, ) splits = text_splitter.split_documents(docs) if splits: print(\u0026#34;成功splits文档。\u0026#34;) else: print(\u0026#34;splits文档失败。\u0026#34;) Docx2txtLoader 类:\n功能: 用于加载 .docx 格式的 Word 文档内容，并将其转换为 LangChain 的 Document 对象列表。每个对象包含从 Word 文档中提取的纯文本内容。 常用参数: file_path: 要加载的 .docx 文件路径（字符串，必填）。 encoding: 文本编码方式（可选，默认为 None，通常不需要设置）。 常用方法: .load(): 读取文档并返回一个 List[Document]，每个 Document 的 page_content 属性包含提取出的文本内容。 RecursiveCharacterTextSplitter 类:\n功能: 将长文本按指定字符递归切分，生成多个较小的文本块（chunks），适合模型输入。通过设置重叠部分保持上下文连贯性。 常用参数: chunk_size: 每个文本块的最大长度（字符数）。 chunk_overlap: 块与块之间的重叠字符数，用于保持上下文连续。 separators: 分隔符列表，按优先级依次尝试切分（如 [\u0026quot;\\n\\n\u0026quot;, \u0026quot;\\n\u0026quot;, \u0026quot; \u0026quot;, \u0026quot;\u0026quot;]）。 length_function: 用于计算文本长度的函数，默认是 len。 常用方法: .split_documents(documents): 接收 List[Document]，返回分割后的 List[Document]。 .split_text(text): 直接对字符串进行分割，返回 List[str]。 4.1.2 步骤2：文本向量化与向量库构建 # 为了将文本转换为向量表示，我们需要一个嵌入模型。这里自定义 AliyunEmbeddings 类来调用 DashScope 的嵌入服务 ：\nfrom langchain.embeddings.base import Embeddings from langchain.vectorstores import Chroma # 自定义阿里云嵌入模型（需先初始化 client） class AliyunEmbeddings(Embeddings): def embed_query(self, text): response = self.client.embeddings.create( input=text, model=\u0026#34;text-embedding-v3\u0026#34; # 百炼文本嵌入模型 ) return response.data[0].embedding def embed_documents(self, texts): return [self.embed_query(text) for text in texts] # 构建向量数据库（首次运行时创建，后续可加载） embeddings = AliyunEmbeddings(client=client) vectordb = Chroma.from_documents( documents=splits, embedding=embeddings, persist_directory=\u0026#34;docs/chroma/\u0026#34; # 本地持久化路径 ) langchain.embeddings.base.Embeddings 类:\n功能说明：Embeddings 是 LangChain 中定义嵌入模型（文本向量化）行为的基类。它是一个抽象接口，用于统一各种嵌入模型的调用方式。所有具体的嵌入模型（如 OpenAI 的text-embedding-ada-002、百炼的 text-embedding-v3、HuggingFace 的本地模型等）都必须实现这个接口中定义的两个核心方法：\n.embed_query(): 对单个字符串进行嵌入。 .embed_documents(): 对多个字符串列表进行嵌入。 LangChain 使用这个接口来支持多种嵌入模型，并让它们在向量数据库（如 Chroma、FAISS 等）中统一使用。\n向量数据库Chroma：\nChroma 是一个轻量级、本地运行的向量数据库，主要用于存储文档及其对应的向量表示（embedding），支持根据语义快速检索相似内容。它在 RAG（检索增强生成）系统中非常常用，可以高效地帮助大模型从知识库中查找相关信息 。\n构建或加载向量数据库 ：\nfrom langchain.vectorstores import Chroma persist_directory =\u0026#34;docs/chroma/\u0026#34; # 初始化你的 embedding 类 embeddings = AliyunEmbeddings(client=client, model=\u0026#34;text-embedding-v3\u0026#34;) # 构建向量数据库（首次运行） vectordb = Chroma.from_documents( documents=splits, # 这是你之前分割好的 splits 文档 embedding=embeddings, persist_directory=persist_directory ) print(\u0026#34;向量数据库构建完成并保存至：\u0026#34;, persist_directory) # 或者 加载已有向量数据库（后续运行时） # vectordb = Chroma(persist_directory=persist_directory,embedding_function=embeddings) Chroma 核心方法包括:\nChroma.from_documents(): 从文档构建向量数据库。 Chroma(): 加载已有数据库。 .similarity_search(): 根据问题查找最相关的文档。 .add_documents(): 向已有数据库中添加新文档。 .as_retriever(): 将 Chroma 实例封装为 LangChain 的 Retriever，用于集成到链式流程中。 4.2 RAG 问答链实现 # 接下来，实现 RAG（检索增强生成）查询功能，结合向量数据库检索和 LLM 生成 ：\n#QA问答 - 营销咨询专家系统 from IPython.display import display, Markdown, clear_output import time def marketing_consulting(query): \u0026#34;\u0026#34;\u0026#34;提供专业营销建议\u0026#34;\u0026#34;\u0026#34; try: # 1. 从向量数据库中检索相关文档 retrieved_docs = vectordb.similarity_search(query, k=3) # 2. 构建上下文内容 context = \u0026#34;\\n\\n\u0026#34;.join([f\u0026#34;📚 来源 {i+1}：{doc.page_content}\u0026#34; for i, doc in enumerate(retrieved_docs)]) # 3. 构造符合Qwen格式的messages messages = [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: ( \u0026#34;您现在是营销领域首席专家，请基于知识库内容提供专业建议。要求：\\n\u0026#34; \u0026#34;1. 分点说明核心策略\\n\u0026#34; \u0026#34;2. 包含具体数据支撑\\n\u0026#34; \u0026#34;3. 给出可执行方案\\n\u0026#34; \u0026#34;4. 营销术语专业准确\u0026#34; ) }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: f\u0026#34;营销主题：{query}\\n\\n参考资料：\\n{context}\u0026#34; } ] # 4. 调用大模型获取专业回答 response = client.chat.completions.create( model=\u0026#34;qwen-turbo\u0026#34;, # 使用最新最强模型 messages=messages, temperature=0.3, # 降低随机性确保专业性 max_tokens=1500 ) answer = response.choices[0].message.content # 5. 增强Markdown显示效果 clear_output(wait=True) display(Markdown(f\u0026#34;### 营销主题：**{query}**\u0026#34;)) display(Markdown(\u0026#34;### 专业建议\u0026#34;)) display(Markdown(answer)) return True except Exception as e: display(Markdown(f\u0026#34;**系统错误**：`{str(e)}`\u0026#34;)) return False # 主交互循环 display(Markdown(\u0026#34;## 营销专家咨询系统已启动\u0026#34;)) while True: user_query = input(\u0026#34;\\n请输入营销主题（输入\u0026#39;退出\u0026#39;结束）：\u0026#34;).strip() if user_query.lower() in [\u0026#39;退出\u0026#39;, \u0026#39;exit\u0026#39;, \u0026#39;quit\u0026#39;]: display(Markdown(\u0026#34;## 感谢使用营销专家系统！\u0026#34;)) break if not user_query: display(Markdown(\u0026#34;⚠ **提示**：请输入有效查询内容\u0026#34;)) continue # 添加查询处理动画 display(Markdown(f\u0026#34;正在分析：**{user_query}** ...\u0026#34;)) start_time = time.time() success = marketing_consulting(user_query) if success: elapsed = time.time() - start_time display(Markdown(f\u0026#34;⏱ 分析耗时：{elapsed:.1f}秒 | 字符数：{len(user_query)}\u0026#34;)) 5. Deepseek 模型实战 # 5.1 Ollama 本地模型部署 # 登录官网 下载 拉取 DeepSeek 模型 ollama pull deepseek-r1:7b # 70亿参数版本，适合本地推理 5.2 LangChain 集成 Ollama 模型 # ChatOllama 类详解\nChatOllama 类用于连接本地通过 Ollama 框架运行的大语言模型（LLM），并调用其进行文本生成、对话交互等任务。该类实现了 LangChain 的 BaseChatModel 接口，支持标准的 LLM 调用方式 。\n安装:\n!pip install -U langchain-ollama 使用示例:\nfrom langchain_community.chat_models import ChatOllama ollama_llm = ChatOllama(model=\u0026#34;deepseek-r1:7b\u0026#34;) 参数说明:\n参数名 类型 默认值 说明 model str \u0026quot;\u0026quot;llama2\u0026quot;\u0026quot; 使用的模型名称及标签，如 \u0026quot;\u0026quot;deepseek-r1:7b\u0026quot;\u0026quot; temperature float 0.8 控制输出随机性（0~1，数值越低回答越确定） max_tokens int None 最大输出 token 数量限制 top_p float 0.9 Nucleus sampling 参数，控制采样范围 streaming bool FALSE 是否启用流式输出（逐字生成） 常用方法：\n方法名 功能说明 .invoke(input) 同步调用模型生成回复，输入为字符串或包含 prompt 的字典 .stream(input) 流式调用模型，逐字返回输出（需设置 streaming=True） .batch(inputs) 批处理多个输入请求 .generate() 生成多个回复候选（可用于高级采样） 5.3 构建 FAISS 向量库 # OllamaEmbeddings 类详解\nOllamaEmbeddings 类用于调用 Ollama 提供的嵌入模型，将文本转换为向量表示（embedding），常用于构建向量数据库（如 FAISS）以支持语义检索（RAG 架构中的关键部分） 。\n常用参数:\n参数名 类型 默认值 说明 model str \u0026quot;\u0026quot;nomic-embed-text:latest”\u0026quot; 使用的嵌入模型名称 show_progress bool False 是否显示进度条（适用于批量嵌入） 常用方法:\n方法名 功能说明 .embed_query(text) 将单个文本字符串转换为 embedding 向量（列表形式） .embed_documents(texts) 将多个文本字符串批量转换为 embedding 向量列表 FAISS\nFAISS 是一个基于 Facebook AI 提供的向量库，实现高效的相似度检索。它支持快速近似最近邻搜索（ANN）以及保存和加载本地索引。\n构建向量数据库 ：\nfrom langchain_huggingface import HuggingFaceEmbeddings # 加载本地 Embedding 模型 embedding_model = HuggingFaceEmbeddings( model_name=\u0026#34;bge-large-zh-v1.5\u0026#34;, # 改为你本地模型路径 ) # 第四步：创建向量数据库 from langchain_community.vectorstores import FAISS vector_store = FAISS.from_documents(split_docs, embedding_model) FAISS 主要方法:\nfrom_documents: 从文档集合创建 FAISS 数据库。 as_retriever(): 创建一个检索器对象，用于 RAG 流程。 5.4 构建 PromptTemplate # 构建自定义的 PromptTemplate 用于问答系统 ：\n#构建promptTemplate from langchain_core.prompts import PromptTemplate custom_prompt = PromptTemplate( template=\u0026#34;\u0026#34;\u0026#34;你是一个专业的业绩数据分析助手，请根据以下上下文回答用户问题：上下文：{context}问题：{question}请用中文简洁明了地回答，如果无法从数据中找到答案，请说明。同时根据用户提出的问题和知识库中的内容给出三个用户最可能关系的问题。\u0026#34;\u0026#34;\u0026#34;, input_variables=[\u0026#34;context\u0026#34;,\u0026#34;question\u0026#34;], ) 5.5 检索问答链：RetrievalQA # RetrievalQA 类结合检索器和 LLM 构建问答链（RAG） 。\n主要参数:\nllm: 使用的语言模型。 chain_type: 控制如何将检索结果传递给 LLM，常用值包括 \u0026quot;stuff\u0026quot;、\u0026quot;map_reduce\u0026quot;、\u0026quot;refine\u0026quot;。 retriever: 用来从向量数据库中检索相关文档（通常来自 vector store）。 chain_type_kwargs: 传递给底层 Chain 的参数，比如 prompt。 return_source_documents: 是否返回检索到的原始文档。 chain_type 对比:\n名称 作用 特点 \u0026quot;stuff\u0026quot; 将所有检索到的文档内容一次性“塞进” prompt 中，供 LLM 使用 1. 最简单、最直接。2. 适合文档数量少、内容短的情况。优点：响应快、上下文完整。缺点：容易超 prompt 上限（如 4096 token）。 \u0026quot;map_reduce\u0026quot; 先对每个文档单独生成答案（map），再将多个答案合并成最终答案（reduce） 1. 适用于文档多或内容长的场景。2. 可以避免 prompt 超长问题。优点：可处理大量数据。缺点：多次调用 LLM，较慢；可能丢失上下文关联。 \u0026quot;refine\u0026quot; 逐步优化答案：先基于第一个文档生成答案，然后依次用后续文档更新答案 1. 逐条处理文档，动态更新答案。2. 适合信息分散、需要综合判断的场景。优点：逻辑更连贯，答案质量更高。缺点：调用 LLM 多次，速度慢。 创建 RetrievalQA 链:\n#创建retrievelQA链 from langchain.chains import RetrievalQA qa_chain = RetrievalQA.from_chain_type( llm=bailian_llm, # 使用百炼 LLM chain_type=\u0026#34;stuff\u0026#34;, retriever=vector_store.as_retriever(search_kwargs={\u0026#34;k\u0026#34;: 3}), chain_type_kwargs={\u0026#34;prompt\u0026#34;: custom_prompt}, return_source_documents=True ) 注意：bailian_llm 在此示例中需要预先定义，例如：\n#加载模型 from langchain_openai import ChatOpenAI from openai import OpenAI bailian_llm = ChatOpenAI( model=\u0026#34;deepseek-r1\u0026#34;, # 百炼支持的模型名称，例如 qwen-turbo 或 qwen-plus api_key=\u0026#34;\u0026lt;your-api-key\u0026gt;\u0026#34;, base_url=\u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#34;, temperature=0.7, max_tokens=512 ) 5.6 运行问答系统 # 运行问答系统并格式化输出结果 ：\n#运行问答系统 def format_response(result): print(\u0026#34;\\n【回答】\u0026#34;) print(result[\u0026#34;result\u0026#34;]) print(\u0026#34;\\n【参考来源】\u0026#34;) seen = set() for i, doc in enumerate(result[\u0026#34;source_documents\u0026#34;][:3], 1): identifier = f\u0026#34;{doc.metadata[\u0026#39;source\u0026#39;]}-{doc.metadata.get(\u0026#39;page\u0026#39;,\u0026#39;\u0026#39;)}\u0026#34; if identifier not in seen: print(f\u0026#34;[来源{i}] {identifier}\u0026#34;) seen.add(identifier) while True: question = input(\u0026#34;\\n请输入问题（输入q退出）：\u0026#34;) if question.lower() ==\u0026#39;q\u0026#39;: break try: result = qa_chain.invoke({\u0026#34;query\u0026#34;: question}) format_response(result) except Exception as e: print(f\u0026#34;发生错误：{str(e)}\u0026#34;) 5.6 完整代码 # from langchain_community.document_loaders import Docx2txtLoader from langchain_text_splitters import RecursiveCharacterTextSplitter from langchain_huggingface import HuggingFaceEmbeddings from langchain_community.vectorstores import FAISS from langchain_core.prompts import PromptTemplate from langchain.chains import RetrievalQA # 1. 文档处理（同前文） loader = Docx2txtLoader(\u0026#34;database/营销话术库.docx\u0026#34;) docs = loader.load() text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=40) splits = text_splitter.split_documents(docs) # 2. 加载本地嵌入模型（如 BGE 中文模型） embeddings = HuggingFaceEmbeddings( model_name=\u0026#34;BAAI/bge-large-zh-v1.5\u0026#34;, # 需提前下载到本地 model_kwargs={\u0026#34;device\u0026#34;: \u0026#34;cpu\u0026#34;} # 可选 \u0026#34;cuda\u0026#34; 加速 ) # 3. 构建 FAISS 向量库（适合大规模数据） vector_store = FAISS.from_documents(splits, embeddings) # 4. 定义提示词模板 prompt = PromptTemplate( template=\u0026#34;\u0026#34;\u0026#34; 你是业绩数据分析助手，请根据上下文回答问题： 上下文：{context} 问题：{question} 要求：简洁回答，若无法回答请说明，并推荐3个相关问题。 \u0026#34;\u0026#34;\u0026#34;, input_variables=[\u0026#34;context\u0026#34;, \u0026#34;question\u0026#34;] ) # 5. 创建检索问答链（使用 refine 策略优化答案） qa_chain = RetrievalQA.from_chain_type( llm=llm, chain_type=\u0026#34;refine\u0026#34;, retriever=vector_store.as_retriever(k=5), chain_type_kwargs={\u0026#34;prompt\u0026#34;: prompt}, return_source_documents=True ) # 6. 运行问答系统 def format_answer(result): print(\u0026#34;\\n【答案】\u0026#34;, result[\u0026#34;result\u0026#34;]) print(\u0026#34;\\n【参考来源】\u0026#34;, [doc.metadata[\u0026#34;source\u0026#34;] for doc in result[\u0026#34;source_documents\u0026#34;][:3]]) while True: query = input(\u0026#34;请输入问题（输入 q 退出）：\u0026#34;) if query.lower() == \u0026#39;q\u0026#39;: break try: result = qa_chain.invoke({\u0026#34;query\u0026#34;: query}) format_answer(result) except Exception as e: print(f\u0026#34;错误：{str(e)}\u0026#34;) 6. LangSmith # LangSmith 是一个用于构建生产级 LLM 应用程序的平台。\n它包含调试、测试、评估和监控基于任何 LLM 框架构建的链和智能代理，并无缝集成 LangChain（用于构建 LLM 的首选开源框架）。\n6.1 新建项目 # 配置环境变量 LANGSMITH_TRACING=true LANGSMITH_ENDPOINT=\u0026#34;https://api.smith.langchain.com\u0026#34; LANGSMITH_API_KEY=\u0026#34;\u0026lt;your-api-key\u0026gt;\u0026#34; LANGSMITH_PROJECT=\u0026#34;问答测试\u0026#34; #项目名 OPENAI_API_KEY=\u0026#34;\u0026lt;your-openai-api-key\u0026gt;\u0026#34; 运行程序完成项目创建 添加@traceable 使得langsmith 可以跟踪程序 from langchain_openai import ChatOpenAI from zhipuai import ZhipuAI from langsmith import traceable zhipu_client = ZhipuAI(api_key=\u0026#34;\u0026lt;your-api-key\u0026gt;\u0026#34;) @traceable def glm(): messages = [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: ( \u0026#34;- Role: 营销策略顾问\\n\u0026#34; \u0026#34;- Background: 用户需要专业的营销建议，以提升产品或服务的市场表现。\\n\u0026#34; \u0026#34;- Profile: 你是一位经验丰富的营销专家，对市场趋势、消费者行为和营销渠道有深刻的理解。\\n\u0026#34; \u0026#34;- Skills: 你具备市场分析、消费者心理洞察、品牌建设、数字营销和传统营销的综合能力。\\n\u0026#34; \u0026#34;- Goals: 提供针对性的营销策略，帮助用户提高品牌知名度、增加客户参与度和提升销售业绩。\\n\u0026#34; \u0026#34;- Constrains: 建议应基于市场研究和数据分析，同时考虑成本效益和可执行性。\\n\u0026#34; \u0026#34;- OutputFormat: 提供具体的营销策略、执行步骤和预期结果的详细报告。\\n\u0026#34; \u0026#34;- Workflow:\\n\u0026#34; \u0026#34; 1. 了解用户的产品或服务特性，以及目标市场和客户群体。\\n\u0026#34; \u0026#34; 2. 分析市场趋势和竞争对手的营销活动。\\n\u0026#34; \u0026#34; 3. 根据用户的需求和市场情况，制定个性化的营销策略。\\n\u0026#34; \u0026#34; 4. 提供执行策略的具体步骤和时间表。\\n\u0026#34; \u0026#34; 5. 预测策略的潜在效果，并提供优化建议。\u0026#34; ) }, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;user_input\u0026#34;}, ] response = zhipu_client.chat.completions.create( model=\u0026#34;glm-4-flash\u0026#34;, messages=messages, ) return response print(glm()) 6.2 建立数据集 # from langsmith import Client, wrappers from openevals.llm import create_llm_as_judge from openevals.prompts import CORRECTNESS_PROMPT from openai import OpenAI # Define the input and reference output pairs that you\u0026#39;ll use to evaluate your app client = Client() # Create the dataset dataset = client.create_dataset( dataset_name=\u0026#34;Test\u0026#34;, description=\u0026#34;A sample dataset in LangSmith.\u0026#34; ) # Create examples in the dataset. Examples consist of inputs and reference outputs examples = [ { \u0026#34;inputs\u0026#34;: {\u0026#34;question\u0026#34;: \u0026#34;对业务需求进行清洗、筛选的指标有哪些\u0026#34;}, \u0026#34;outputs\u0026#34;: {\u0026#34;answer\u0026#34;: \u0026#34;①需求实现的难易及复杂程度；②需求实现的时间周期；③需求实现的成本；④需求的轻重缓.\u0026#34;}, }, { \u0026#34;inputs\u0026#34;: {\u0026#34;question\u0026#34;: \u0026#34;在数字化建设初期传统企业存在哪些困惑与担心？\u0026#34;}, \u0026#34;outputs\u0026#34;: { \u0026#34;answer\u0026#34;: \u0026#34;①担心数字化落地的效果 --- 落地难；②企业管理缺乏标准化的能力 --- 管理难；③担心员工素质低无法承受数字化的专业技术能力 ---推广难；④担心系统改变了原有的工作模式，在应用过程中受阻 --- 应用难；⑤担心数字化系统过于复杂、专业，员工难以适应，成为工作负担--- 操作难；⑥不知道如何推广数字化系统 --- 认知难；以上是当前部分传统企业的常见问题，究其原因最主要的还是四个“缺乏”：①缺乏对数字化的深度认知；②缺乏转型的魄力；③缺乏数字化的专业领导人才；④缺乏数字化基础能力；\u0026#34;}, }, { \u0026#34;inputs\u0026#34;: {\u0026#34;question\u0026#34;: \u0026#34;如何开展数字化对标学习？\u0026#34;}, \u0026#34;outputs\u0026#34;: { \u0026#34;answer\u0026#34;: \u0026#34;①带：带目的、带问题、带诚意；②看： 第一看,组织管理能力；第二看，技术与业务的协同能力；第三看，对IT的投入支持能力；第四看，对数据的深入应用能力；第五看，踩了多少坑，趟了多少雷；③学：学其文化、学其方法、学其措施；④定：定班子、定团队、定规划、定路线、定投入、定标准、定责任、定绩效；\u0026#34;}, }, ] # Add the examples to the dataset client.create_examples(dataset_id=dataset.id, examples=examples) 6.3 评估问答 # 6.3.1 添加配置信息 # import os os.environ[\u0026#39;LANGCHAIN_TRACING_V2\u0026#39;] = \u0026#39;true\u0026#39; os.environ[\u0026#39;LANGCHAIN_ENDPOINT\u0026#39;]=\u0026#34;https://api.smith.langchain.com\u0026#34; os.environ[\u0026#39;LANGCHAIN_API_KEY\u0026#39;]=\u0026#34;\u0026#34; # langsmith的api_key os.environ[\u0026#39;LANGCHAIN_PROJECT\u0026#39;]=\u0026#34;问答测试\u0026#34; 获取创建的数据集链接\nfrom langsmith import evaluate, Client from langsmith.schemas import Example, Run import os from zhipuai import ZhipuAI os.environ[\u0026#39;LANGCHAIN_API_KEY\u0026#39;]=\u0026#34;\u0026#34;# langsmith的api_key client = Client() dataset =client.clone_public_dataset(\u0026#34;\u0026#34;)#填入数据集链接 zhipu_client = ZhipuAI(api_key=\u0026#34;\u0026#34;)#填入智谱apikey 6.3.2 定义评估器 # #根据用户输入结合知识库生成生成针对性的营销策略 def pipeline(user_input: str): messages = [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: ( \u0026#34;- Role: 营销策略顾问\\n\u0026#34; \u0026#34;- Background: 用户需要专业的营销建议，以提升产品或服务的市场表现。\\n\u0026#34; \u0026#34;- Profile: 你是一位经验丰富的营销专家，对市场趋势、消费者行为和营销渠道有深刻的理解。\\n\u0026#34; \u0026#34;- Skills: 你具备市场分析、消费者心理洞察、品牌建设、数字营销和传统营销的综合能力。\\n\u0026#34; \u0026#34;- Goals: 提供针对性的营销策略，帮助用户提高品牌知名度、增加客户参与度和提升销售业绩。\\n\u0026#34; \u0026#34;- Constrains: 建议应基于市场研究和数据分析，同时考虑成本效益和可执行性。\\n\u0026#34; \u0026#34;- OutputFormat: 提供具体的营销策略、执行步骤和预期结果的详细报告。\\n\u0026#34; \u0026#34;- Workflow:\\n\u0026#34; \u0026#34; 1. 了解用户的产品或服务特性，以及目标市场和客户群体。\\n\u0026#34; \u0026#34; 2. 分析市场趋势和竞争对手的营销活动。\\n\u0026#34; \u0026#34; 3. 根据用户的需求和市场情况，制定个性化的营销策略。\\n\u0026#34; \u0026#34; 4. 提供执行策略的具体步骤和时间表。\\n\u0026#34; \u0026#34; 5. 预测策略的潜在效果，并提供优化建议。\u0026#34; ) }, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_input}, ] tools = [ { \u0026#34;type\u0026#34;: \u0026#34;retrieval\u0026#34;, \u0026#34;retrieval\u0026#34;: { \u0026#34;knowledge_id\u0026#34;: \u0026#34;1854410905543143424\u0026#34;, # 知识库ID \u0026#34;prompt_template\u0026#34;: ( \u0026#34;从文档\\n\\\u0026#34;\\\u0026#34;\\\u0026#34;\\n{{knowledge}}\\n\\\u0026#34;\\\u0026#34;\\\u0026#34;\\n中找问题\\n\\\u0026#34;\\\u0026#34;\\\u0026#34;\\n\u0026#34; \u0026#34;{{question}}\\n\\\u0026#34;\\\u0026#34;\\\u0026#34;\\n的答案，找到答案就仅使用文档语句回答问题，并且对输出格式进行整理美化；\u0026#34; \u0026#34;找不到答案就用自身知识回答并且告诉用户该信息不是来自文档。\\n\u0026#34; \u0026#34;不要复述问题，直接开始回答。\u0026#34; ), }, } ] response = zhipu_client.chat.completions.create( model=\u0026#34;glm-4-flash\u0026#34;, messages=messages, tools=tools, ) return response.choices[0].message.content # 评估两个文本的相似度。 def rateResult(generate: str, reference: str): messages = [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: ( \u0026#34;- Role: 文本相似度评估专家\\n\u0026#34; \u0026#34;- Background: 用户需要对比两个文段，即“答案”和“参考内容”，以评估它们之间的相似度和准确性。\\n\u0026#34; \u0026#34;- Profile: 你是一位专业的文本分析专家，擅长通过比较和对照不同文本内容，准确评估它们之间的相似度和一致性。\\n\u0026#34; \u0026#34;- Skills: 你具备文本解析、语义理解、信息比对和评分系统设计的能力，能够根据预设标准对文本相似度进行量化评估。\\n\u0026#34; \u0026#34;- Goals: 根据“答案”与“参考内容”的相似度和准确性，给出1至10的量化得分。\\n\u0026#34; \u0026#34;- Constrains: 评估必须基于客观标准，确保评分的公正性和一致性。\\n\u0026#34; \u0026#34;- OutputFormat: 返回一个1至10的得分，代表“答案”与“参考内容”的相似度。\\n\u0026#34; \u0026#34;- Workflow:\\n\u0026#34; \u0026#34; 1. 仔细阅读并理解“答案”和“参考内容”。\\n\u0026#34; \u0026#34; 2. 比较两个文段的主题、关键信息和细节描述。\\n\u0026#34; \u0026#34; 3. 根据相似度评分标准，确定“答案”与“参考内容”的相似度得分。\\n\u0026#34; \u0026#34; 4. 只需要输出得分，不用输出别的\\n\u0026#34; \u0026#34;- Examples:\\n\u0026#34; \u0026#34; - 答案：“苹果是一种红色的水果。”\\n\u0026#34; \u0026#34; 参考内容：“苹果是一种常见的水果，通常呈红色或绿色。”\\n\u0026#34; \u0026#34; 7\\n\u0026#34; \u0026#34; - 答案：“水的分子式是H2O。”\\n\u0026#34; \u0026#34; 参考内容：“水的分子式是H2O，是最简单的氧化物之一。”\\n\u0026#34; \u0026#34; 7\\n\u0026#34; \u0026#34; - 答案：“地球是太阳系的第三颗行星。”\\n\u0026#34; \u0026#34; 参考内容：“地球是太阳系的第三颗行星，也是唯一已知存在生命的行星。”\\n\u0026#34; \u0026#34; 7\u0026#34; ) }, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;答案是：\u0026#34; + generate + \u0026#34;，参考内容是\u0026#34; + reference}, ] response = zhipu_client.chat.completions.create( model=\u0026#34;glm-4-flash\u0026#34;, messages=messages, ) return response.choices[0].message.content # Define an evaluator def is_concise_enough(root_run: Run, example: Example) -\u0026gt; dict: score = rateResult(root_run.outputs[\u0026#34;output\u0026#34;], example.outputs[\u0026#34;answer\u0026#34;]) return {\u0026#34;key\u0026#34;: \u0026#34;is_concise\u0026#34;, \u0026#34;score\u0026#34;: int(score)} 6.3.3 运行评估 # result=evaluate( lambda x: pipeline(x[\u0026#34;question\u0026#34;]), data=dataset.name, evaluators=[is_concise_enough], experiment_prefix=\u0026#34;my experiment\u0026#34; ) result ","date":"5 June 2025","externalUrl":null,"permalink":"/byteglow/posts/langchain%E7%AE%80%E6%98%93%E6%95%99%E7%A8%8B/","section":"Posts","summary":"","title":"LangChain 简易教程","type":"posts"},{"content":"","externalUrl":null,"permalink":"/byteglow/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/byteglow/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/byteglow/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]